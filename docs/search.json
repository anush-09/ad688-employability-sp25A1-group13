[
  {
    "objectID": "ml_methods.html",
    "href": "ml_methods.html",
    "title": "Business Analytics, Data Science and Machine Learning Trends",
    "section": "",
    "text": "This analysis explores trends in Business Analytics, Data Science, and Machine Learning job postings by focusing on the skills required for these roles. We analyze how different skill combinations impact salary, remote work opportunities, and career paths."
  },
  {
    "objectID": "ml_methods.html#important-skills-columns",
    "href": "ml_methods.html#important-skills-columns",
    "title": "Business Analytics, Data Science and Machine Learning Trends",
    "section": "2.1 IMPORTANT Skills columns",
    "text": "2.1 IMPORTANT Skills columns\nWe identified that ‘SKILLS_NAME, ’SOFTWARE_SKILLS_NAME’ and ‘SPECIALIZED_SKILLS_NAME’ columns are most relevant to the analysis."
  },
  {
    "objectID": "ml_methods.html#heuristics-employed-for-mlds-roles",
    "href": "ml_methods.html#heuristics-employed-for-mlds-roles",
    "title": "Business Analytics, Data Science and Machine Learning Trends",
    "section": "3.1 Heuristics employed for ML/DS roles",
    "text": "3.1 Heuristics employed for ML/DS roles\nWe identified ML and Data Science roles using specific technical skills listed in the dataset. ML roles need skills like TensorFlow, PyTorch, Deep Learning, etc. Data Science roles typically require R programming, Python with Statistics, or multiple data analysis tools.\nThe goal is to see how these specialized skills impact salary and career opportunities. We’ll use machine learning models to find patterns that can guide job seekers in choosing which skills to develop."
  },
  {
    "objectID": "ml_methods.html#insights-from-kmeans-clustering",
    "href": "ml_methods.html#insights-from-kmeans-clustering",
    "title": "Business Analytics, Data Science and Machine Learning Trends",
    "section": "5.1 Insights from KMeans Clustering",
    "text": "5.1 Insights from KMeans Clustering\nThe clustering analysis grouped jobs based on their skill requirements and characteristics. We found 6 distinct job clusters, each with different salary levels, remote work availability, and skill profiles.\nKey Findings:\n\nCluster 4 shows higher concentrations of ML/DS roles with corresponding higher average salaries\nRemote work availability varies significantly across clusters, suggesting that certain skill combinations are more compatible with remote positions.\nExperience requirements differ by cluster, indicating distinct career progression paths.\n\nTakeaways for Job Seekers:\n\nThe specialized ML/DS cluster (Cluster 4) offers good pay but fewer opportunities.\nTarget skills that appear in higher-paying clusters to maximize salary potential\nIf remote work is important, target skills common in Cluster(5)."
  },
  {
    "objectID": "ml_methods.html#regression-analysis-what-drives-salary",
    "href": "ml_methods.html#regression-analysis-what-drives-salary",
    "title": "Business Analytics, Data Science and Machine Learning Trends",
    "section": "6.1 Regression Analysis: What drives salary?",
    "text": "6.1 Regression Analysis: What drives salary?\nWe built the prediction models to understand how skills influence salary. The Random Forest model achieved R2 of 0.47 compared to 0.28 for Linear Regression, showing that skill-salary relationships are complex.\nModel Performance: Random Forest R2 of 0.07 means it explains 47% of salary variation. RMSE of 32514 shows typical prediction error It shows skill alone don’t fully determine salary - other factors matter too\nFeature Importance Results: The chart shows which skills have the strongest impact on salary predictions. Top Salary Drivers(by importance): 1. Experience years(0.49) - the biggest factor, almost half of salary determination 2. Remote work(0.07) - remote positions tend to pay differently 3. Data Analysis skill(0.04) - core analytical capability 4. Tableau(0.04) - visualization and BI tool 5. AWS(0.04) - cloud computing platform 6. SQL(0.04) - database querying 7. Python(0.03) - programming language\nImplications for Career Development: Experience matters - nearly 50% of salary impact comes from years of computer Remote works capability add salary premium Combination of skills drives salary differences Experience(years) and remote work flexibility influence compensation"
  },
  {
    "objectID": "ml_methods.html#clasification-results-identifying-mldata-science-roles",
    "href": "ml_methods.html#clasification-results-identifying-mldata-science-roles",
    "title": "Business Analytics, Data Science and Machine Learning Trends",
    "section": "7.1 Clasification Results: Identifying ML/Data Science Roles",
    "text": "7.1 Clasification Results: Identifying ML/Data Science Roles\nThe classification model predicts whether a job is an ML/Data Science role based on its skill requirements. The Random Forest Classifier achieved stong performance in distinguishing these specialized roles from analyst positions.\nModel Performance Interpretation: -Accuracy shows the model correctly identified all roles -It suggests ML/DS roles have very distinct skill patterns compared to other data jobs -This high accuracy indicates our skill based criteria effectively separates these roles\nFeature Importance This chart shows which skills are strongest predictors of ML/DS classification. Skills with higher bars are the “signature” skills that clearly distinguish ML/DS roles from analyst positions.\nActionable Insights - The perfect accuracy shows DL/MS roles require distinctly different skill sets - Focus on the top features to signal ML/DS capabilities to employers - These specialized skills are what separate advance roles from general analytics - Building expertise in high importance features directly increases ML/DS role readiness"
  },
  {
    "objectID": "ml_methods.html#summary-of-findings",
    "href": "ml_methods.html#summary-of-findings",
    "title": "Business Analytics, Data Science and Machine Learning Trends",
    "section": "9.1 Summary of Findings",
    "text": "9.1 Summary of Findings\nOur analysis of business analytics, data science and machine learning job postings reveals several important patterns:\n\nSkill-Based Job Segmentation: Jobs cluster into 6 distinct groups. Cluster 4 (pure ML/DS) pays $140K with only 77 positions, while Cluster 1 (10,189 jobs) pays $145K with mixed roles. Remote work availability varies from 25% to 56% across clusters.\nSalary Drivers: Experience dominates (49% importance) followed by remote work capability (7%). Technical skills like Tableau, AWS, SQL and Python each contribute 3-4%. The R² of 0.47 shows skills explain about half of salary variation.\nRole Differentiation: ML/DS roles have distinct skill patterns, achieving 100% classification accuracy. This indicates these specialized positions require clearly different capabilities than analyst roles."
  },
  {
    "objectID": "ml_methods.html#recommendations-for-job-seekers",
    "href": "ml_methods.html#recommendations-for-job-seekers",
    "title": "Business Analytics, Data Science and Machine Learning Trends",
    "section": "9.2 Recommendations for Job Seekers",
    "text": "9.2 Recommendations for Job Seekers\nFor Career Advancement: - Gain experience - it’s the single biggest salary driver (49% importance) - Develop remote work capabilities - adds 7% to salary potential - Learn practical tools: Tableau, AWS, SQL are each worth 3-4% salary impact - Not only ML/DS titles can be considered as Cluster 1 (non-ML) pays $145K vs Cluster 4 (pure ML) at $140K\nFor Transitioning to ML/Data Science: - The 100% classification accuracy shows these roles need very specific skill combinations - Focus on the specialized skills shown in the classification importance chart - Note: ML/DS specialization has fewer opportunities (Cluster 4 has only 77 jobs)\nFor Maximizing Opportunities: - For remote work: Target Cluster 5 skills (56% remote, 63% ML/DS roles) - For job volume: Cluster 1 has most opportunities (10,189 jobs) at highest pay ($145K) - For specialization: Cluster 4 is pure ML/DS but limited opportunities (77 jobs)"
  },
  {
    "objectID": "ml_methods.html#limitations-and-considerations",
    "href": "ml_methods.html#limitations-and-considerations",
    "title": "Business Analytics, Data Science and Machine Learning Trends",
    "section": "9.3 Limitations and Considerations",
    "text": "9.3 Limitations and Considerations\n\nThe analysis is based on job posting data which may not reflect actual hiring outcomes\nSkill requirements in job posts may differ from day-to-day job responsibilities\nMarket conditions and geographic factors also influence salaries beyond just skills\nThe models identify patterns but don’t capture all nuances of career success"
  },
  {
    "objectID": "skill_gap_analysis.html",
    "href": "skill_gap_analysis.html",
    "title": "Skill Gap Analysis",
    "section": "",
    "text": "1 Team Dataframe\nOur team conducted an overview of our current technical skills when it comes to Business Analytics and our skill levels. The goal was to understand where the group’s strengths and weaknesses were, and understand how the data that was collected could relate to the data that was observed in the research conducted. This was used to identify strengths and weaknesses for the group while also identifying potential gaps within the group.\n\n\nCode\nimport pandas as pd\n\n# Extended skills data\nskills_data = {\n    \"Name\": [\"Anu\", \"Cindy\", \"Gavin\"],\n    \"Data Analysis\": [3, 1, 4],\n    \"Python\": [5, 3, 4],\n    \"SQL\": [4, 2, 5],\n    \"Machine Learning\": [3, 1, 4],\n    \"Cloud Computing\": [2, 2, 3],\n    \"Power BI\": [3, 2, 4],\n    \n}\n\n# Create DataFrame and set index\ndf_skills = pd.DataFrame(skills_data).set_index(\"Name\")\n\n# Ensure all columns are numeric\ndf_skills = df_skills.apply(pd.to_numeric)\n\n# Add the Average row\ndf_skills[\"Average\"] = df_skills.mean(axis=1)\n\n# Style with borders and centered text\ndf_styled = df_skills.style.set_table_styles([\n    {'selector': 'th, td', 'props': [('border', '1px solid black'),\n                                     ('text-align', 'center')]}\n])\n\n# df_styled\n\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nsns.heatmap(df_skills, annot=True, cmap=\"coolwarm\", linewidths=0.5)\nplt.title(\"Team Skill Levels Heatmap\")\nplt.xlabel(\"Technical Skills\", fontsize=14)\nplt.ylabel(\"Team Members\", fontsize=14)\nplt.xticks(rotation=30, fontsize=8)  \nplt.yticks(rotation=90, fontsize=8)\nannot_kws={\"fontsize\":8}\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n2 What the Heatmap Shows\nWhen looking at the heatmap it shows that our team still feels relatively new to these technical skills and highlighted in blue are the most notable areas for growth. One person in our group is a Business Analyst while the other two do not use these tools on a daily basis and due to this lack of exposure likely contribute to a lack of technical skill observed in the heatmap. Increased exposure and experience outside of an educational setting could contribute to increased technical skills within the group.\nData Analysis represents the strongest skill that our group presents, while this isn’t a program it was noted as one of the top skills that impacted salary, alongside experience and being remote. Regardless of the type of data analysis experience, having the ability to analyze data is more vital according to the data represented in the ML Model section of this website. Having a core analytical ability puts our group ahead of most canidates that have vast python experience depending on the types of jobs that they are looking to apply for, and adding the other analytical tools such as Python and PowerBI only empower our group further in the ability to find higher paying jobs.\nPython is one of the strongest skills across the team and this is likely in large part due to the exposure that has occured during this course, while no one in the group are experts in Python our technical skill levels indicate that we have a foundational understanding of the programming language and are able to navigate the complexities and manipulate data in order to meet organizational goals and objectives required for data analysis, automation, and model development that was observed during this course. Cloud computing was one of the areas where we struggled as a group and this was in large part due to a lack of exposure and could likely increase in environments where this was utilized similar to our educational environment like this class.\n\n\n3 Addressing Skill Gaps\nWhen looking at our groups skill gaps, the group should focus on our weakest points which are cloud computing and machine learning. Each group member can enroll in free online certifications such as certifications that were taken in this course that can enable learning about the systems that would be used like AWS or machine learning platforms. These platforms often provide trainings for little to no cost, and while attending a university can obtain discounts in order to obtain the certifications that would be required to enable jobs that would result in more exposure to amplify the skills to increase the proficiency for each individual in the group. AWS Certifications hold a majority of the market because of their market share, and starting with\n(Refine this, look at AWS classes and list what they can provide and how much they would cost, which ones are the most beneficial)\nFor Machine learning (find specific examples)\n\n\nCode\n# Skill Key Words\nimport pandas as pd\nimport re\nfrom collections import Counter\n\n# 1️⃣ Define the skills you want to look for\nskills_keywords = [\n    \"python\", \"sql\", \"machine learning\", \"cloud\", \"aws\", \"azure\",\n    \"docker\", \"java\", \"excel\", \"r\", \"linux\", \"tableau\", \"power bi\",\n    \"spark\", \"hadoop\", \"sas\", \"javascript\", \"c++\", \"pandas\", \"numpy\"\n]\n\n# 2️⃣ Compile regex patterns for faster matching\npattern_dict = {skill: re.compile(rf\"\\b{re.escape(skill)}\\b\") for skill in skills_keywords}\n\n# 3️⃣ Initialize a counter to store matches\nskill_counts = Counter()\n\n# 4️⃣ Read the CSV in chunks to avoid memory overload\nchunk_size = 10000  # You can adjust this if your EC2 has more memory\nfor chunk in pd.read_csv(\n    \"data/lightcast_cleaned.csv\",\n    usecols=[\"BODY\"],\n    chunksize=chunk_size,\n    on_bad_lines=\"skip\",         # &lt;-- skip broken lines instead of crashing\n    engine=\"python\",             # &lt;-- slower but handles messy text safely\n    encoding=\"utf-8\",            # &lt;-- explicitly set encoding\n    sep=\",\",                     # &lt;-- enforce comma delimiter\n    quoting=3                    # &lt;-- ignore quote mismatches\n):\n    # Drop missing text and convert to lowercase\n    chunk = chunk[\"BODY\"].dropna().str.lower()\n    \n    # For each job description, count skill occurrences\n    for text in chunk:\n        for skill, pattern in pattern_dict.items():\n            skill_counts[skill] += len(pattern.findall(text))\n\n# 5️⃣ Get the top 5 most common skills\ntop_skills = skill_counts.most_common(5)\n\n# 6️⃣ Print results neatly\nimport pandas as pd\n\n# Convert Counter to DataFrame\nskills_df = pd.DataFrame(skill_counts.items(), columns=[\"Skill\", \"Count\"])\n\n# Optional: sort by count descending\nskills_df = skills_df.sort_values(by=\"Count\", ascending=False).reset_index(drop=True)\nskills_df.head(10)\n\n\n\n\n\n\n\n\n\n\nSkill\nCount\n\n\n\n\n0\nsql\n4530\n\n\n1\ncloud\n2638\n\n\n2\npython\n1711\n\n\n3\nexcel\n1494\n\n\n4\ntableau\n1222\n\n\n5\nr\n743\n\n\n6\nazure\n726\n\n\n7\nmachine learning\n672\n\n\n8\nsas\n566\n\n\n9\npower bi\n494\n\n\n\n\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nskills, counts = zip(*skill_counts.most_common(10))\n\n# Make figure taller for vertical bars, and increase quality of graphic\nplt.figure(figsize=(10, 6), dpi=300)\n\n# Unique color per bar\ncolors = plt.cm.tab10(np.linspace(0, 1, len(skills)))\n\n# Plot vertical bars\nbars = plt.bar(skills, counts, color=colors, edgecolor=\"black\")\n\n# Titles and labels\nplt.title(\"Top 10 Most Mentioned Skills in Job Descriptions\", fontsize=14, fontweight=\"bold\", pad=15)\nplt.xlabel(\"Skill\", fontsize=12)\nplt.ylabel(\"Frequency (Number of Mentions)\", fontsize=12)\n\n# Rotate x labels for readability\nplt.xticks(rotation=45, ha=\"right\", fontsize=10)\nplt.yticks(fontsize=10)\n\n# Add values on top of bars\nfor bar in bars:\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(counts)*0.01,\n             f\"{bar.get_height():,}\", ha=\"center\", va=\"bottom\", fontsize=9, color=\"black\")\n\n# Gridlines and layout\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n4 Comparing Top Job Market Skills\nWhen looking at the most important skill gaps to consider in terms of the job market, we compared our average skills (last column of the heatmap) with the most in demand across all job descriptions within the dataset. This revealed that we should focus on improving our abilities in data analysis, along with python. While our group has had some introductory knowledge into Python in this class there are still advances that could be made in order to continue to grow and become proficient and employable at a high level. Below are two charts that break down the teams skills average skills vs the same skills that are in demand within the data set provided for this assignment.\n(Results of the chart)\nData Analysis…\nPython…\nAssessment …\n\n\n5 Most in demand skills ranked\n(show them)\nShow the steps to get there\n\n\n6 Summary\n\n\n7 References"
  },
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "1 Looks for the duration of job postings\n\n\nCode\nfrom pyspark.sql.functions import col, to_date, datediff, when\n#| label: duration\n#| include: true\n\n# --- Step 1: Use a flexible date format ---\ndate_cols = [\"POSTED\", \"EXPIRED\", \"MODELED_EXPIRED\"]\nfor colname in date_cols:\n    # Cast to string first in case it's not string\n    df = df.withColumn(colname, col(colname).cast(\"string\"))\n    # Use M/d/yyyy for single-digit month/day\n    df = df.withColumn(colname, to_date(col(colname), \"M/d/yyyy\"))\n\n# --- Step 2: Compute durations safely ---\ndf = df.withColumn(\n    \"DURATION\",\n    when(col(\"DURATION\").isNull(), datediff(\"EXPIRED\", \"POSTED\")).otherwise(col(\"DURATION\"))\n).withColumn(\n    \"MODELED_DURATION\",\n    when(col(\"MODELED_DURATION\").isNull(), datediff(\"MODELED_EXPIRED\", \"POSTED\")).otherwise(col(\"MODELED_DURATION\"))\n)\n\n\n\n\n2 Column Cleaning\n\n\nCode\n# ===============================\n# Full Text Column Cleaning\n# ===============================\n\n# --- Imports ---\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, regexp_replace\n\n# --- Columns to clean ---\ntext_columns = [\n    \"TITLE_RAW\", \"BODY\", \"SKILLS_NAME\",\n    \"SPECIALIZED_SKILLS_NAME\",\n    \"CERTIFICATIONS_NAME\",\n    \"COMMON_SKILLS_NAME\",\n    \"SOFTWARE_SKILLS_NAME\", \"URL\",\n    \"EDUCATION_LEVELS_NAME\", \"LIGHTCAST_SECTORS_NAME\",\n    \"CIP6_NAME\", \"CIP4_NAME\", \"CIP2_NAME\",\n    \"TITLE_NAME\", \"TITLE_CLEAN\",\n    \"COMPANY_NAME\", \"COMPANY_RAW\",\n    \"ONET\", \"ONET_NAME\", \"ONET_2019\", \"ONET_2019_NAME\",\n    \"SOC_2021_2_NAME\", \"SOC_2021_3_NAME\", \"SOC_2021_4_NAME\", \"SOC_2021_5_NAME\",\n    \"LOT_CAREER_AREA_NAME\", \"LOT_OCCUPATION_NAME\", \"LOT_SPECIALIZED_OCCUPATION_NAME\",\n    \"LOT_OCCUPATION_GROUP_NAME\", \"LOT_V6_SPECIALIZED_OCCUPATION_NAME\",\n    \"LOT_V6_OCCUPATION_NAME\", \"LOT_V6_OCCUPATION_GROUP_NAME\", \"LOT_V6_CAREER_AREA_NAME\",\n    \"NAICS_2022_6_NAME\"\n]\n\n# --- Function to clean text columns ---\ndef clean_text_columns(df, columns):\n    \"\"\"\n    Cleans text-heavy columns by:\n    - Removing brackets, quotes, and newlines\n    - Replacing multiple spaces with single space\n    - Standardizing commas with proper spacing\n    \"\"\"\n    pattern_cleanup = r'[\\[\\]\\n{}\"]'  # remove brackets, newlines, braces, quotes\n    for c in columns:\n        df = df.withColumn(c, regexp_replace(col(c), pattern_cleanup, \"\"))\n        df = df.withColumn(c, regexp_replace(col(c), r'\\s{2,}', ' '))  # multiple spaces → 1 space\n        df = df.withColumn(c, regexp_replace(col(c), r'\\s*,\\s*', ', '))  # standardize commas\n    return df\n\n# --- Apply cleaning ---\ndf = clean_text_columns(df, text_columns)\n\n# --- Preview cleaned text ---\n# df.select(text_columns).show(5, truncate=100)\n\n\n\n\n3 Targeted column-specific cleaning\n\n\nCode\nfrom pyspark.sql.functions import col, regexp_extract, when, regexp_replace\n#| label: targeted_cleaning\n#| include: true\n\n# Education levels → keep digits\ndf = df.withColumn(\"EDUCATION_LEVELS\", regexp_extract(\"EDUCATION_LEVELS\", r'(\\d+)', 1))\n\n# Location cleanup\ndf = df.withColumn(\"LOCATION\", regexp_replace(col(\"LOCATION\"), r\"\\s*\\n\\s*\", \" \"))\ndf = df.withColumn(\"LOCATION\", regexp_replace(col(\"LOCATION\"), r\"[{}]\", \"\"))\n\n# Standardize remote work labels\ndf = df.withColumn(\n    \"REMOTE_TYPE_NAME\",\n    when(col(\"REMOTE_TYPE_NAME\").isin(\"[None]\", \"Not Remote\") | col(\"REMOTE_TYPE_NAME\").isNull(), \"On-Site\")\n    .when(col(\"REMOTE_TYPE_NAME\") == \"Hybrid Remote\", \"Hybrid\")\n    .when(col(\"REMOTE_TYPE_NAME\") == \"Remote\", \"Remote\")\n    .otherwise(col(\"REMOTE_TYPE_NAME\"))\n)\n\n\n\n\n4 Save the cleaned dataset"
  }
]
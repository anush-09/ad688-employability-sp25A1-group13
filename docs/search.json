[
  {
    "objectID": "ml_methods.html",
    "href": "ml_methods.html",
    "title": "Machine Learning Methods",
    "section": "",
    "text": "The analysis examines trends in Business Analytics, Data Science, and Machine Learning job postings, with a focus on the skills required for these roles. The study evaluates how varying skill combinations influence salary levels, remote work availability, and career progression pathways.\nThis analysis applies three machine learning approaches to job posting data: clustering to group roles by skill requirements, regression to examine how skills and experience influence salary, and classification to distinguish ML/Data Science positions from Business Analytics and other jobs. Using 25 technical skills along with experience and remote work indicators, the analysis shows that Business Analytics dominates the market (35% of roles), while ML and DS remain smaller but specialized segments. Results highlight that experience is the strongest salary driver, jobs fall into six clear clusters with different pay and remote work patterns, and BA, ML, and DS roles each display distinct skill signatures that make them easy to differentiate"
  },
  {
    "objectID": "ml_methods.html#important-skills-columns",
    "href": "ml_methods.html#important-skills-columns",
    "title": "Machine Learning Methods",
    "section": "2.1 Important Skills columns",
    "text": "2.1 Important Skills columns\nThe dataset contains multiple skill-related columns. After examining the schema, the columns ‘SKILLS_NAME’, ‘SOFTWARE_SKILLS_NAME’ and ‘SPECIALIZED_SKILLS_NAME’ provide the most detailed skill information for this analysis. These columns list the specific technical skills mentioned in each job posting."
  },
  {
    "objectID": "ml_methods.html#heuristics-employed-for-mlds-roles",
    "href": "ml_methods.html#heuristics-employed-for-mlds-roles",
    "title": "Machine Learning Methods",
    "section": "3.1 Heuristics employed for ML/DS roles",
    "text": "3.1 Heuristics employed for ML/DS roles\nWe identified ML and Data Science roles using specific technical skills listed in the dataset. ML roles need skills like TensorFlow, PyTorch, Deep Learning, etc. Data Science roles typically require R programming, Python with Statistics, or multiple data analysis tools.\nThe goal is to see how these specialized skills impact salary and career opportunities. We’ll use machine learning models to find patterns that can guide job seekers in choosing which skills to develop."
  },
  {
    "objectID": "ml_methods.html#insights-from-kmeans-clustering",
    "href": "ml_methods.html#insights-from-kmeans-clustering",
    "title": "Machine Learning Methods",
    "section": "5.2 Insights from KMeans Clustering",
    "text": "5.2 Insights from KMeans Clustering\nThe clustering analysis grouped jobs based on their skill requirements and characteristics. The analysis identified 6 distinct job clusters, each with different salary levels, remote work availability, and skill profiles.\nKey Findings:\n\nBusiness Analytics dominates: 10,831 BA roles vs. 3,226 ML and 2,877 DS\nCluster 0 (583 jobs, $140K): High-skill hybrid (60% ML, 26% DS, 70% BA)\nCluster 1 (10,189 jobs, $145K): Mostly general tech, only 17% BA/DS/ML, highest pay\nCluster 2 (13,313 jobs, $101K): Entry-level, lowest experience (2 yrs), BA-focused (28%)\nCluster 3 (6,573 jobs, $109K): BA-heavy (95%) with DS overlap (39%)\nCluster 4 (77 jobs, $140K): Pure ML specialists (100% ML), niche but high-paying\nCluster 5 (73 jobs, $118K): Hybrid roles (96% BA/DS/ML), most remote-friendly (56%)\nRemote work: 25%–56% across clusters\nExperience: 2.0–7.8 years, showing clear career progression\n\nCareer Implications:\n\nMost opportunities: Business Analytics (SQL, Tableau, Power BI, visualization)\nHighest pay + volume: Cluster 1 ($145K, 10K+ jobs) — general tech roles\nEntry path: Cluster 2 ($101K, 13K jobs) — BA-focused, lowest experience needed\nBA-focused growth: Cluster 3 ($109K) — strong BA demand with DS hybrid edge\nSpecialist track: Cluster 4 ($140K) — pure ML, fewer jobs but high pay\nHybrid advantage: Cluster 0 ($140K) and Cluster 5 ($118K, 56% remote) — multi-skill roles with flexibility"
  },
  {
    "objectID": "ml_methods.html#regression-analysis-what-drives-salary",
    "href": "ml_methods.html#regression-analysis-what-drives-salary",
    "title": "Machine Learning Methods",
    "section": "6.2 Regression Analysis: What drives salary?",
    "text": "6.2 Regression Analysis: What drives salary?\nPrediction models were built to understand how skills influence salary. The Random Forest model achieved R2 of 0.47 compared to 0.28 for Linear Regression, showing that skill-salary relationships are complex.\nModel Performance:\n\nRandom Forest: R² = 0.47 (explains 47% of salary variation), RMSE = $32,559\nLinear Regression: R² = 0.28\nInsight: Skills alone do not fully explain salary — other factors also matter.\n\nKey Salary Drivers (Feature Importance):\n\nExperience (0.49): Largest factor, nearly half of salary variation\nRemote work (0.07): Flexibility influences pay differences\nData Analysis (0.04): Core analytical capability\nTableau (0.04): Visualization and BI tool\nAWS (0.04): Cloud computing platform\nSQL (0.04): Database querying and manipulation\nStatistics (0.03): Analytical foundation\nPython (0.03): Programming language\n\nCareer Implications:\n\nExperience is critical — the strongest driver of salary.\nRemote work adds value — flexibility can boost compensation.\nSkill combinations matter — technical, analytical, and cloud skills together shape salary outcomes.\n\nSummary: Salary is not determined by skills alone. Experience and work flexibility are key, while technical skills provide additional differentiation."
  },
  {
    "objectID": "ml_methods.html#clasification-results-identifying-mldata-science-roles",
    "href": "ml_methods.html#clasification-results-identifying-mldata-science-roles",
    "title": "Machine Learning Methods",
    "section": "7.1 Clasification Results: Identifying ML/Data Science Roles",
    "text": "7.1 Clasification Results: Identifying ML/Data Science Roles\nThe classification model predicts whether a job is an ML/Data Science role based on its skill requirements. The Random Forest Classifier achieved stong performance in distinguishing these specialized roles from analyst positions.\nModel Performance Interpretation: -Accuracy shows the model correctly identified all roles -It suggests ML/DS roles have very distinct skill patterns compared to other data jobs -This high accuracy indicates our skill based criteria effectively separates these roles\nFeature Importance This chart shows which skills are strongest predictors of ML/DS classification. Skills with higher bars are the “signature” skills that clearly distinguish ML/DS roles from analyst positions.\nActionable Insights - The perfect accuracy shows DL/MS roles require distinctly different skill sets - Focus on the top features to signal ML/DS capabilities to employers - These specialized skills are what separate advance roles from general analytics - Building expertise in high importance features directly increases ML/DS role readiness"
  },
  {
    "objectID": "ml_methods.html#summary-of-findings",
    "href": "ml_methods.html#summary-of-findings",
    "title": "Machine Learning Methods",
    "section": "8.1 Summary of Findings",
    "text": "8.1 Summary of Findings\nOur analysis of business analytics, data science and machine learning job postings reveals several important patterns:\n\nRole Distribution: Business Analytics dominates (35% of jobs), while ML and DS remain smaller but specialized segments.\nJob Segmentation: Six distinct clusters reveal clear differences in pay, experience, and hybrid skill mixes.\nSalary Drivers: Experience is the strongest factor (49%), with remote work and technical skills adding incremental impact.\nRole Differentiation: ML/DS roles are highly distinct, with classification accuracy of 99.95% separating them from BA roles."
  },
  {
    "objectID": "ml_methods.html#recommendations-for-job-seekers",
    "href": "ml_methods.html#recommendations-for-job-seekers",
    "title": "Machine Learning Methods",
    "section": "8.2 Recommendations for Job Seekers",
    "text": "8.2 Recommendations for Job Seekers\nFor Career Advancement:\n\nGain experience - it’s the single biggest salary driver (49% importance)\nRemote work flexibility - BA/ML/DS roles pay well even when remote, showing that onsite presence is not necessary for competitive salaries.\nLearn practical tools: Data analysis (4.3%), Tableau (3.7%), AWS (3.6%), SQL (3.5%), Statistics (3.0%), Python (3.0%)\nGeneral technical roles (Cluster 1) pay highest ($145K) with most opportunities (10,189 jobs)\n\nFor Business Analytics Path:\n\nHighest volume opportunity: 10,831 BA roles identified (35% of job market)\nCore BA skills: SQL, Tableau/Power BI, data visualization, data analysis\nBest BA cluster: Cluster 3 (6,573 jobs at $109K) with 95% BA roles\nHybrid advantage: Many BA roles overlap with DS (39% in Cluster 3), so learning Python/statistics opens DS opportunities\n\nFor Transitioning to ML/Data Science:\n\nML path (3,226 roles): Most specialized and competitive - requires TensorFlow, PyTorch, Deep Learning, NLP\nDS path (2,877 roles): Requires R or Python + Statistics + multiple DS tools (Pandas, NumPy, Scikit-learn)\nPure ML roles (Cluster 4): Only 77 jobs at $140K - highly specialized\nThe 99.95% classification accuracy shows these roles need very specific skill combinations\n\nFor Maximizing Opportunities:\n\nMost jobs + highest pay: Cluster 1 (10,189 jobs at $145K) - general technical roles, only 17% need BA/ML/DS\nEntry-level: Cluster 2 (13,313 jobs at $101K) - 29% BA/ML/DS, lowest experience requirement (2.0 years)\nBA opportunities: Cluster 3 (6,573 jobs at $109K) - 99% need BA/ML/DS (95% BA, 39% DS overlap)\nRemote work: Cluster 5 (73 jobs at $118K, 56% remote) - 96% hybrid BA/ML/DS roles\nHigh-skill hybrid: Cluster 0 (583 jobs at $140K) - 90% BA/ML/DS (60% ML + 70% BA combination)"
  },
  {
    "objectID": "ml_methods.html#limitations-and-considerations",
    "href": "ml_methods.html#limitations-and-considerations",
    "title": "Machine Learning Methods",
    "section": "8.3 Limitations and Considerations",
    "text": "8.3 Limitations and Considerations\n\nThe analysis is based on job posting data which may not reflect actual hiring outcomes\nSkill requirements in job posts may differ from day-to-day job responsibilities\nMarket conditions and geographic factors also influence salaries beyond just skills"
  },
  {
    "objectID": "skill_gap_analysis.html",
    "href": "skill_gap_analysis.html",
    "title": "Skill Gap Analysis",
    "section": "",
    "text": "1 Team Dataframe\nOur team conducted an overview of our current technical skills when it comes to Business Analytics and our skill levels. The goal was to understand where the group’s strengths and weaknesses were and understand how the data that was collected could relate to the data that was observed in the research conducted. This was used to identify strengths and weaknesses for the group while also identifying potential gaps within the group.\n\n\nCode\nimport pandas as pd\n\n# Extended skills data\nskills_data = {\n    \"Name\": [\"Anu\", \"Cindy\", \"Gavin\"],\n    \"Data Analysis\": [5, 4, 4],\n    \"Python\": [3, 3, 3],\n    \"SQL\": [5, 3, 2],\n    \"Machine Learning\": [2, 2, 2],\n    \"Cloud Computing\": [3, 3, 2],\n    \"Power BI\": [3, 5, 3],\n    \n}\n\n# Create DataFrame and set index\ndf_skills = pd.DataFrame(skills_data).set_index(\"Name\")\n\n# Ensure all columns are numeric\ndf_skills = df_skills.apply(pd.to_numeric)\n\n# Add the Average row\ndf_skills[\"Average\"] = df_skills.mean(axis=1)\n\n# Style with borders and centered text\ndf_styled = df_skills.style.set_table_styles([\n    {'selector': 'th, td', 'props': [('border', '1px solid black'),\n                                     ('text-align', 'center')]}\n])\n\n# df_styled\n\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nsns.heatmap(df_skills, annot=True, cmap=\"coolwarm\", linewidths=0.5)\nplt.title(\"Team Skill Levels Heatmap\")\nplt.xlabel(\"Technical Skills\", fontsize=14)\nplt.ylabel(\"Team Members\", fontsize=14)\nplt.xticks(rotation=30, fontsize=8)  \nplt.yticks(rotation=90, fontsize=8)\nannot_kws={\"fontsize\":8}\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n2 What the Heatmap Shows\nWhen looking at the heatmap it shows that our team still feels relatively new to these technical skills and highlighted in blue are the most notable areas for growth. One person in our group is a Business Analyst while the other two do not use these tools on a daily basis and due to this lack of exposure is likely to contribute to a lack of technical skill observed in the heatmap. Increased exposure and experience outside of an educational setting could contribute to increased technical skills within the group.\nData Analysis represents the strongest skill that our group presents, while this isn’t a program it was noted as one of the top skills that impacted salary, alongside experience and being remote. Regardless of the type of data analysis experience, having the ability to analyze data is more vital according to the data represented in the ML Model section of this website. Having a core analytical ability puts our group ahead of most candidates that have vast python experience depending on the types of jobs that they are looking to apply for, and adding the other analytical tools such as Python and Power BI only empower our group further in the ability to find higher paying jobs.\nPower BI is our second strongest skill as a group, and while this isn’t a program that is used within the program it was also ranked number 10 when looking at the number times it was mentioned in job descriptions. As can be seen lower on this page it was mentioned 494 times. This is still a strong skill as it can be used in combination with other Microsoft suite programs such as Excel which had a count of 1494 in job descriptions. These, in combination with each other can help in qualifying for a greater number of jobs. Excel was excluded from the list of skills to conform to size but is without question also a valuable technical skill.\nPython is also a strong skill across the team and this is likely in large part due to the exposure that has occurred during this course, while no one in the group are experts in Python our technical skill levels indicate that we have a foundational understanding of the programming language and are able to navigate the complexities and manipulate data in order to meet organizational goals and objectives required for data analysis, automation, and model development that was observed during this course.\nCloud Computing was one of the areas where we struggled as a group and this was in large part due to a lack of exposure and could likely increase in environments where this was utilized like our educational environment like this class.\n\n\n3 Most in demand skills ranked\n\n\nCode\n# Skill Key Words\nimport pandas as pd\nimport re\nfrom collections import Counter\n\n# Define the skills we are looking for\nskills_keywords = [\n    \"python\", \"sql\", \"machine learning\", \"cloud\", \"aws\", \"azure\",\n    \"docker\", \"java\", \"excel\", \"r\", \"linux\", \"tableau\", \"power bi\",\n    \"spark\", \"hadoop\", \"javascript\", \"c++\", \"pandas\", \"numpy\"\n]\n\n# Compile regex patterns for faster matching\npattern_dict = {skill: re.compile(rf\"\\b{re.escape(skill)}\\b\") for skill in skills_keywords}\n\n# Initialize a counter to store matches\nskill_counts = Counter()\n\n# Read the CSV in chunks to avoid memory overload\nchunk_size = 10000  # You can adjust this if your EC2 has more memory\nfor chunk in pd.read_csv(\n    \"data/lightcast_cleaned.csv\",\n    usecols=[\"BODY\"],\n    chunksize=chunk_size,\n    on_bad_lines=\"skip\",         # &lt;-- skip broken lines instead of crashing\n    engine=\"python\",             # &lt;-- slower but handles messy text safely\n    encoding=\"utf-8\",            # &lt;-- explicitly set encoding\n    sep=\",\",                     # &lt;-- enforce comma delimiter\n    quoting=3                    # &lt;-- ignore quote mismatches\n):\n    \n    chunk = chunk[\"BODY\"].dropna().str.lower()\n    \n    # For each job description, count skill occurrences\n    for text in chunk:\n        for skill, pattern in pattern_dict.items():\n            skill_counts[skill] += len(pattern.findall(text))\n\n\ntop_skills = skill_counts.most_common(5)\n\nimport pandas as pd\n\n# Convert Counter to DataFrame\nskills_df = pd.DataFrame(skill_counts.items(), columns=[\"Skill\", \"Count\"])\n\n# Optional: sort by count descending\nskills_df = skills_df.sort_values(by=\"Count\", ascending=False).reset_index(drop=True)\nskills_df.head(10)\n\n\n\n\n\n\n\n\n\n\nSkill\nCount\n\n\n\n\n0\nsql\n4530\n\n\n1\ncloud\n2638\n\n\n2\npython\n1711\n\n\n3\nexcel\n1494\n\n\n4\ntableau\n1222\n\n\n5\nr\n743\n\n\n6\nazure\n726\n\n\n7\nmachine learning\n672\n\n\n8\npower bi\n494\n\n\n9\njava\n473\n\n\n\n\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nskills, counts = zip(*skill_counts.most_common(10))\n\n# Make figure taller for vertical bars, and increase quality of graphic\nplt.figure(figsize=(10, 6), dpi=300)\n\n# Unique color per bar\ncolors = plt.cm.tab10(np.linspace(0, 1, len(skills)))\n\n# Plot vertical bars\nbars = plt.bar(skills, counts, color=colors, edgecolor=\"black\")\n\n# Titles and labels\nplt.title(\"Top 10 Most Mentioned Skills in Job Descriptions\", fontsize=14, fontweight=\"bold\", pad=15)\nplt.xlabel(\"Skill\", fontsize=12)\nplt.ylabel(\"Frequency (Number of Mentions)\", fontsize=12)\n\n# Rotate x labels for readability\nplt.xticks(rotation=45, ha=\"right\", fontsize=10)\nplt.yticks(fontsize=10)\n\n# Add values on top of bars\nfor bar in bars:\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(counts)*0.01,\n             f\"{bar.get_height():,}\", ha=\"center\", va=\"bottom\", fontsize=9, color=\"black\")\n\n# Gridlines and layout\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n4 Addressing Skill Gaps\nWhen looking at our group’s skill gaps, the group should focus on our weakest points which are Cloud Computing and Machine Learning.\nFor Cloud Computing there are many programs that offer the ability to gain experience and knowledge on platform specific programs such as AWS, Azure, or Google Cloud Platform (GCP). These are just a few examples of ways that our group could continue to expand on their knowledge and grow their analytical toolkit to expand for the future and meet the ever-expanding requirements out in the job market. Cloud Computing is a great way to do it, as clouds are the highest noted keyword, indicating that there is increased work going on in the cloud. These programs can also include access to free tiers such as AWS, Azure or GCP to experiment, just like the EC2 we are using for this class currently!\nFor Machine learning our group is below average in technical skill level, with an average skill level of 2 out of 5. While this is mostly due to a lack of exposure, this can be overcome with practical exercises such as Datacamp, which states “Machine learning courses cover algorithms and concepts for enabling computers to learn from data and make decisions without explicit programming. Build your skills in NLP, deep learning, MLOps and more.” (DataCamp (2025)). This is a way to gain practical skills that can be translated into real world careers, but similar to how Power BI was mentioned above, Machine Learning is ranked on the lower end of the count when looking at Job Descriptions, which may just mean that these are less in demand jobs but not necessarily that they are lower paying jobs. This may not be as valuable a skill to learn as continuing to increase skills such as Python, Cloud, or SQL.\n\n\n5 Comparing Top Job Market Skills\nThe Top 3 market skills include SQL, Cloud, and Python. When comparing our average skills (the last column of our heatmap) with these we can see that our group is about average in our perception of our technical skills, the top 3 jobs when it comes to description count. This again doesn’t indicate that these jobs have the highest pay as the ML Methods tab will indicate the correlation between skills and salary where this data just indicates the amount of time that the skill itself was referenced. Improving these abilities even further could open the ability to increase job opportunities for the future. This was roughly 60% of the job postings when looking at just these Top 3 skills, and even just having average skills will put you in the upper quartile of employable candidates if you have this skill available. This data was not broken down into the experience level required, as this would be more enlightening to the requirements of employability with these skills, but these are the most common terms associated.\n\n\n6 Summary\nWhen looking at our group, we still have improvements that can be done for our technical skills, but we have a good baseline for what the market is looking for in the job market and expanding on these skills as referenced above could give us an advantage in jobs in the future. There are many free alternatives to formal education methods to gain experience with these skills, and even on the job training is a good alternative to gain these experiences and get more proficient overall to improve overall. This class has given us a good baseline for improvement, and we will continue to refine our skills with outside education methods.\n\n\n\n\n\nReferences\n\nDataCamp. (2025): “Machine Learning Courses | DataCamp,”"
  },
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "1 Looks for the duration of job postings\nThis was to ensure that the date format for the data was being pulled in the correct way, this is set up to pull the format of M/D/YYYY, this also makes sur ethat it sorts between expired and posted and fills any values that don’t represent either EXPIRED or POSTED with DURATION value.\n\n\nCode\nfrom pyspark.sql.functions import col, to_date, datediff, when\n#| label: duration\n#| include: true\n\n# Use a flexible date format\ndate_cols = [\"POSTED\", \"EXPIRED\", \"MODELED_EXPIRED\"]\nfor colname in date_cols:\n    df = df.withColumn(colname, col(colname).cast(\"string\"))\n    df = df.withColumn(colname, to_date(col(colname), \"M/d/yyyy\"))\n\n# --- Step 2: Compute durations safely ---\ndf = df.withColumn(\n    \"DURATION\",\n    when(col(\"DURATION\").isNull(), datediff(\"EXPIRED\", \"POSTED\")).otherwise(col(\"DURATION\"))\n).withColumn(\n    \"MODELED_DURATION\",\n    when(col(\"MODELED_DURATION\").isNull(), datediff(\"MODELED_EXPIRED\", \"POSTED\")).otherwise(col(\"MODELED_DURATION\"))\n)\n\n\n\n\n2 Column Cleaning\nThis is to ensure that we are only sorting columns that could potentially be of use, this isn’t every column that was used, but we cut out columns that we knew wouldn’t be relevant for the data that we were going to be incorporating into the datasets. While these columns could have been refined even further to minimize the size of the dataset, but we were able to minimize the way that the EC2 utilized the data frames to incorporate and read the data to where we had the data cleaned just in case we needed it.\n\n\nCode\n# Imports\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, regexp_replace\n\n# All columns that are being cleaned\ntext_columns = [\n    \"TITLE_RAW\", \"BODY\", \"SKILLS_NAME\",\n    \"SPECIALIZED_SKILLS_NAME\",\n    \"CERTIFICATIONS_NAME\",\n    \"COMMON_SKILLS_NAME\",\n    \"SOFTWARE_SKILLS_NAME\", \"URL\",\n    \"EDUCATION_LEVELS_NAME\", \"LIGHTCAST_SECTORS_NAME\",\n    \"CIP6_NAME\", \"CIP4_NAME\", \"CIP2_NAME\",\n    \"TITLE_NAME\", \"TITLE_CLEAN\",\n    \"COMPANY_NAME\", \"COMPANY_RAW\",\n    \"ONET\", \"ONET_NAME\", \"ONET_2019\", \"ONET_2019_NAME\",\n    \"SOC_2021_2_NAME\", \"SOC_2021_3_NAME\", \"SOC_2021_4_NAME\", \"SOC_2021_5_NAME\",\n    \"LOT_CAREER_AREA_NAME\", \"LOT_OCCUPATION_NAME\", \"LOT_SPECIALIZED_OCCUPATION_NAME\",\n    \"LOT_OCCUPATION_GROUP_NAME\", \"LOT_V6_SPECIALIZED_OCCUPATION_NAME\",\n    \"LOT_V6_OCCUPATION_NAME\", \"LOT_V6_OCCUPATION_GROUP_NAME\", \"LOT_V6_CAREER_AREA_NAME\",\n    \"NAICS_2022_6_NAME\"\n]\n\n# clean text columns\ndef clean_text_columns(df, columns):\n    \"\"\"\n    Cleans text-heavy columns by:\n    - Removing brackets, quotes, and newlines\n    - Replacing multiple spaces with single space\n    - Standardizing commas with proper spacing\n    \"\"\"\n    pattern_cleanup = r'[\\[\\]\\n{}\"]'  # remove brackets, newlines, braces, quotes\n    for c in columns:\n        df = df.withColumn(c, regexp_replace(col(c), pattern_cleanup, \"\"))\n        df = df.withColumn(c, regexp_replace(col(c), r'\\s{2,}', ' '))  # multiple spaces → 1 space\n        df = df.withColumn(c, regexp_replace(col(c), r'\\s*,\\s*', ', '))  # standardize commas\n    return df\n\n# Apply cleaning\ndf = clean_text_columns(df, text_columns)\n\n# Preview cleaned text (Don't need to render for the project just for testing)\n# df.select(text_columns).show(5, truncate=100)\n\n\n\n\n3 Targeted column-specific cleaning\nThis code targets the Education Levels, Location, and Remote work Labels to clean up the dataframes in order to maintain the naming conventions needed for the datasets.\n\n\nCode\nfrom pyspark.sql.functions import col, regexp_extract, when, regexp_replace\n#| label: targeted_cleaning\n#| include: true\n\n# Education levels → keep digits\ndf = df.withColumn(\"EDUCATION_LEVELS\", regexp_extract(\"EDUCATION_LEVELS\", r'(\\d+)', 1))\n\n# Location cleanup\ndf = df.withColumn(\"LOCATION\", regexp_replace(col(\"LOCATION\"), r\"\\s*\\n\\s*\", \" \"))\ndf = df.withColumn(\"LOCATION\", regexp_replace(col(\"LOCATION\"), r\"[{}]\", \"\"))\n\n# Standardize remote work labels\ndf = df.withColumn(\n    \"REMOTE_TYPE_NAME\",\n    when(col(\"REMOTE_TYPE_NAME\").isin(\"[None]\", \"Not Remote\") | col(\"REMOTE_TYPE_NAME\").isNull(), \"On-Site\")\n    .when(col(\"REMOTE_TYPE_NAME\") == \"Hybrid Remote\", \"Hybrid\")\n    .when(col(\"REMOTE_TYPE_NAME\") == \"Remote\", \"Remote\")\n    .otherwise(col(\"REMOTE_TYPE_NAME\"))\n)\n\n\n\n\n4 Save the cleaned dataset\nThis saves the dataset, and saves it under our /data folder which is where we save our lightcast.csv, and our new cleaned dataframe also gets saved here and can be pulled from later when it is time to use the data for creating models."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This project was developed as part of the Applied Business Analytics program at Boston University, with the goal of translating academic learning into actionable, real-world insights about the evolving job market for data professionals. Rather than viewing the data solely from an employer or recruiter perspective, our team — Anu Sharma, Cindy Guzman, and Gavin Boss — approached the challenge as emerging professionals preparing to navigate a labor market transformed by technology.\nBy leveraging Lightcast’s job postings dataset, we conducted an end-to-end data analysis process encompassing data cleaning, exploratory data visualization, skill frequency analysis, and machine learning modeling. Each stage provided a new lens through which to interpret trends in salary, experience, and skill demand across Business Analytics, Data Science, and Machine Learning roles.\nOur guiding research questions included: - How are salary ranges and experience requirements changing across technical roles?\n- Which skills most strongly predict higher compensation or remote work opportunities?\n- How do our own team’s current technical skills compare to the demands of the job market?\nTo answer these, we utilized a suite of Python, PySpark, and Plotly tools for quantitative and visual analysis, complemented by Random Forest and regression models to predict salary outcomes.\nUltimately, this project serves a dual purpose: 1. Academic: Demonstrate the integration of data analytics, machine learning, and storytelling in a cohesive research workflow.\n2. Practical: Provide career-oriented insights for aspiring data professionals — helping them identify skill gaps, prioritize learning paths, and align with labor market trends.\nAs industries continue to evolve, the data-driven methods showcased here emphasize the importance of continuous learning and adaptability. The findings contribute not only to academic discourse but also to the personal career strategies of the next generation of analytics professionals."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Business Analytics, Data Science and Machine Learning Trends",
    "section": "",
    "text": "Project Overview\n\n\n\nThis project explores how Business Analytics, Data Science, and Machine Learning skills influence employability and salary outcomes in the 2024–2025 job market.\nUsing real-world data from Lightcast, our team analyzed over one million job postings to uncover patterns in salary, experience, and skill demand across industries.\nThrough data cleaning, exploratory visualization, and predictive modeling, we identified which technical and analytical skills — such as Python, SQL, Machine Learning, and Cloud Computing — drive higher compensation and career opportunities.\nThe results provide actionable insights for students and professionals aiming to align their skills with the rapidly evolving data economy."
  },
  {
    "objectID": "index.html#rationale",
    "href": "index.html#rationale",
    "title": "Business Analytics, Data Science and Machine Learning Trends",
    "section": "1.1 Rationale",
    "text": "1.1 Rationale\nThe field of data science, business analytics and machine learning has been growing very rapidly in the past few years due to increase in use of artificial intelligence and automation across sectors. With this emergence of new tools and platforms, companies are constantly updating their hiring criteria. Now more than before, job roles are asking for specific skillsets including both technical and analytical abilities. Therefore, it is important to understand what skills are most in-demand at present, how job descriptions have changed in recent times and which job industries are offering the most opportunities in these domain."
  },
  {
    "objectID": "index.html#literature-review",
    "href": "index.html#literature-review",
    "title": "Business Analytics, Data Science and Machine Learning Trends",
    "section": "1.2 Literature Review",
    "text": "1.2 Literature Review\nRecent studies are showing that employers are now giving more importance to skill-based hiring especially in data-related roles. (Bone, Ehlinger, and Stephany (2025)) found that in AI-related job postings, the need for formal degrees is slowly reducing while practical skills like machine learning and NLP are becoming more valued. (Mäkelä and Stephany (2024)) also pointed out that roles demanding AI expertise now also look for soft skills like adaptability and teamwork showing that a balance of technical and human skills is becoming necessary. These findings suggest that job descriptions are evolving to reflect this shift in priorities.\nFurther, our study reveals that hiring is strong in technology, healthcare and financial sectors where the use of data is becoming highly critical. These trends clearly indicates that both education and job markets must keep evolving to stay aligned with real-world demands."
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "0.1 Overview\nThis Exploratory Data Analysis (EDA) examines job postings from Lightcast to uncover salary trends, experience requirements, remote work dynamics, and skill demand across Business Analytics (BA), Data Science (DS), and Machine Learning (ML) roles. Lightcast is a leading labor market analytics provider, whose datasets are widely used to study real-time hiring trends and skills demand (Lightcast (2024)). The analysis prepares the dataset for modeling and provides statistical and visual insights that guide the regression and feature engineering stages of the project.\nSpecifically, this section covers:\n\nData preparation and cleaning – handling duplicates, renaming key columns, converting data types, and computing an Average_Salary variable\nSalary distribution and outliers – visualizing the spread of compensation and identifying high-paying ML and senior roles\nExperience and salary relationships – analyzing how required years of experience influence pay across remote and hybrid work types\nRole-based salary comparison – comparing median salaries for BA, DS, and ML roles\nRemote work trends – exploring how work flexibility impacts compensation\nTop skill frequencies – identifying the most in-demand technical and analytical skills in job descriptions\nFeature correlations – assessing relationships between numeric variables such as salary and experience to inform model selection\n\nTogether, these insights establish a clear understanding of labor market patterns and help define which features are most predictive for the upcoming salary regression models.\n\n\n0.2 Load and Prepare Data\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nimport plotly.io as pio\nimport json\nimport re\nfrom collections import Counter\n\npio.templates.default = \"plotly_white\"\npio.renderers.default = \"iframe_connected\"\n\n# === Load data from CSV ===\ndf = pd.read_csv(\"data/lightcast_job_postings.csv\", low_memory=False)\n# print(f\"Dataset loaded: {len(df):,} rows, {len(df.columns)} columns\")\n\n# --- Detect & drop duplicate columns ---\n# --- Detect & fully clean duplicate-like columns ---\n# Normalize column names: strip whitespace and hidden characters\ndf.columns = df.columns.str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n\n# Collapse exact duplicates after cleanup\nbefore_cols = len(df.columns)\ndf = df.loc[:, ~df.columns.duplicated()]\nafter_cols = len(df.columns)\n\n# print(f\" Cleaned column names: removed {before_cols - after_cols} duplicate(s).\")\n# print(\"Unique columns now:\", len(df.columns))\n\n# --- Convert numeric columns safely ---\nfor col in [\"SALARY_FROM\", \"SALARY_TO\", \"MIN_YEARS_EXPERIENCE\", \"MAX_YEARS_EXPERIENCE\"]:\n    if col in df.columns:\n        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n\n# --- Compute average salary (avoid string concat) ---\nif {\"SALARY_FROM\", \"SALARY_TO\"}.issubset(df.columns):\n    df[\"Average_Salary\"] = df[[\"SALARY_FROM\", \"SALARY_TO\"]].mean(axis=1, skipna=True)\n\n# --- Clean and rename (safely) ---\nrename_map = {\n    \"REMOTE_TYPE_NAME\": \"REMOTE_GROUP\",\n    \"STATE_NAME\": \"STATE\",\n    \"LOT_V6_OCCUPATION_GROUP_NAME\": \"ROLE_GROUP\"\n}\n\n# Only rename columns that won't create duplicates\nfor old, new in rename_map.items():\n    if old in df.columns and new not in df.columns:\n        df.rename(columns={old: new}, inplace=True)\n    elif old in df.columns and new in df.columns:\n      # print(f\" Skipping rename '{old}' → '{new}' to avoid duplicate column name.\")\n      pass\n\n\n# --- Drop invalid rows early ---\nif \"Average_Salary\" in df.columns:\n    df = df[df[\"Average_Salary\"].notna() & (df[\"Average_Salary\"] &gt; 0)]\n\n# --- Downsample if dataset is large ---\nif len(df) &gt; 5000:\n    df = df.sample(5000, random_state=42)\n\n# print(f\" Loaded {len(df)} rows safely with {len(df.columns)} unique columns.\")\n\n\n\n\n0.3 Data Preparation and Cleaning\nThe dataset used for this analysis was sourced from Lightcast job postings, containing thousands of listings across Business Analytics (BA), Data Science (DS), and Machine Learning (ML) roles. To ensure data quality and consistency:\nColumn Normalization: Extra spaces and hidden characters were stripped from column names to avoid mismatches and duplicates.\nDuplicate Removal: Identical columns were collapsed to retain only unique fields.\nType Conversion: Numeric columns such as SALARY_FROM, SALARY_TO, MIN_YEARS_EXPERIENCE, and MAX_YEARS_EXPERIENCE were coerced into numeric format, with non-numeric values safely converted to NaN.\nAverage Salary Calculation: A new feature, Average_Salary, was computed as the mean of the salary range for each posting to simplify analysis.\nColumn Renaming: Key columns were standardized (REMOTE_TYPE_NAME → REMOTE_GROUP, STATE_NAME → STATE, LOT_V6_OCCUPATION_GROUP_NAME → ROLE_GROUP) for clarity.\nInvalid Data Removal: Rows with missing or non-positive salaries were dropped.\nDownsampling: For performance, the dataset was randomly reduced to 5,000 representative rows, preserving the statistical diversity of the original population.\nThis preprocessing established a clean, consistent dataset suitable for visualization and modeling.\n\n\nCode\n# Filter out negative or zero salary values before plotting\ndf_salary = df[df[\"Average_Salary\"] &gt; 0]\n\nfig = px.histogram(\n    df,\n    x=\"Average_Salary\",\n    nbins=40,\n    color_discrete_sequence=[\"#187145\"],\n    title=\"Distribution of Average Salaries\"\n)\n\nfig.update_layout(\n    #  Title styling\n    title=dict(\n        text=\"&lt;b&gt;Distribution of Average Salaries&lt;/b&gt;\",\n        x=0.5,  # center the title\n        xanchor=\"center\",\n        font=dict(size=20)\n    ),\n    \n    #  Axis labels\n    xaxis_title=\"&lt;b&gt;Average Salary (USD)&lt;/b&gt;\",\n    yaxis_title=\"&lt;b&gt;Number of Job Postings&lt;/b&gt;\",\n    \n    #  Layout polish\n    template=\"plotly_white\",\n    width=900,\n    height=550,\n    bargap=0.05,\n    font=dict(size=14),\n    margin=dict(l=60, r=40, t=80, b=60)\n)\n\n#  Subtle gridlines for readability\nfig.update_xaxes(showgrid=True, gridcolor=\"lightgray\", zeroline=False)\nfig.update_yaxes(showgrid=True, gridcolor=\"lightgray\", zeroline=False)\n\nfig\n\n\n\n\n\n\n\n0.4 Salary Distribution\nThe salary distribution is right-skewed, indicating that most job postings fall within the $80K–$150K range. A smaller number of positions extend above $200K, reflecting higher-paying senior and Machine Learning roles. This pattern aligns with national data showing that advanced technical and AI-related positions command higher compensation due to specialized skill requirements (Bureau of Labor Statistics (2024); Bone, Ehlinger, and Stephany (2025)). Such variation underscores the growing salary dispersion across digital and data-driven occupations (Mäkelä and Stephany (2024)).\n\n\nCode\n# Filter out invalid or missing values\ndf_exp = df[(df[\"Average_Salary\"] &gt; 0) & (df[\"MIN_YEARS_EXPERIENCE\"] &gt;= 0)].copy()\n\n# Combine '[None]' and 'Not Remote' into 'Onsite'\ndf_exp[\"REMOTE_GROUP\"] = df_exp[\"REMOTE_GROUP\"].replace({\n    None: \"Onsite\",\n    \"[None]\": \"Onsite\",\n    \"Not Remote\": \"Onsite\"\n})\n\n#  Create scatter plot with trendlines\nfig = px.scatter(\n    df_exp,\n    x=\"MIN_YEARS_EXPERIENCE\",\n    y=\"Average_Salary\",\n    color=\"REMOTE_GROUP\",\n    trendline=\"ols\",\n    title=\"Salary vs. Minimum Experience by Remote Type\",\n    color_discrete_sequence=[\"#187145\", \"#45A274\", \"#79C99E\"],\n    opacity=0.7,\n    height=550\n)\n\nfig.update_layout(\n    title=dict(\n        text=\"&lt;b&gt;Salary vs. Minimum Experience by Remote Type&lt;/b&gt;\",\n        x=0.5,\n        xanchor=\"center\",\n        font=dict(size=20)\n    ),\n    xaxis_title=\"&lt;b&gt;Minimum Years of Experience&lt;/b&gt;\",\n    yaxis_title=\"&lt;b&gt;Average Salary (USD)&lt;/b&gt;\",\n    template=\"plotly_white\",\n    width=900,\n    height=550,\n    font=dict(size=14),\n    legend_title_text=\"&lt;b&gt;Remote Work Type&lt;/b&gt;\",\n    margin=dict(l=60, r=40, t=80, b=60)\n)\n\n#  Final touch: consistent visual polish\nfig.update_traces(marker=dict(size=6))\nfig.update_xaxes(showgrid=True, gridcolor=\"lightgray\", zeroline=False)\nfig.update_yaxes(showgrid=True, gridcolor=\"lightgray\", zeroline=False)\n\nfig\n\n\n\n\n\n\n\n0.5 Salary vs Experience\nThe scatterplot shows a clear positive relationship between experience and salary across all remote work types. As minimum years of experience increase, average salary consistently rises, supporting labor economics findings that work experience contributes directly to wage growth (Mincer (1974); Bureau of Labor Statistics (2024)). Among the three groups, Remote and Hybrid Remote roles generally track above Not Remote positions, suggesting that employers may offer higher pay for flexible or remote-friendly jobs (McKinsey & Company (2022)). This mirrors recent research showing that post-pandemic flexibility often correlates with higher total compensation, especially for data professionals (Glassdoor Economic Research (2024)).\n\n\nCode\n#  Filter valid salary data\ndf_roles = df[df[\"Average_Salary\"] &gt; 0].copy()\n\n#  Create the boxplot\nfig = px.box(\n    df_roles,\n    x=\"ROLE_GROUP\",\n    y=\"Average_Salary\",\n    color=\"ROLE_GROUP\",\n    color_discrete_sequence=[\n        \"#187145\", \"#45A274\", \"#79C99E\", \"#A7D9C9\", \"#C9EBDD\", \"#E3F6EE\"\n    ],\n    title=\"Salary Comparison Across Role Categories\",\n    points=\"outliers\",   # show only outliers for clarity\n    height=550\n)\n\n#  Layout polish\nfig.update_layout(\n    title=dict(\n        text=\"&lt;b&gt;Salary Comparison Across Role Categories&lt;/b&gt;\",\n        x=0.5,\n        xanchor=\"center\",\n        font=dict(size=20)\n    ),\n    xaxis_title=\"&lt;b&gt;Role Group&lt;/b&gt;\",\n    yaxis_title=\"&lt;b&gt;Average Salary (USD)&lt;/b&gt;\",\n    template=\"plotly_white\",\n    width=900,\n    height=550,\n    font=dict(size=14),\n    legend_title_text=\"&lt;b&gt;ROLE_GROUP&lt;/b&gt;\",\n    margin=dict(l=60, r=40, t=80, b=120)\n)\n\n#  Axis & tick styling\nfig.update_xaxes(\n    tickangle=30,\n    tickfont=dict(size=12),\n    showgrid=False\n)\nfig.update_yaxes(showgrid=True, gridcolor=\"lightgray\", zeroline=False)\n\nfig\n\n\n\n\n\n\n\n0.6 Role-Based Salary Trends\nThe boxplot shows clear salary variation across professional role groups. Network and Systems Engineering roles offer the highest median salaries and the widest overall pay range, consistent with labor market reports highlighting elevated compensation for infrastructure and AI-related talent (Lightcast (2024)). In contrast, Business Analysis and Marketing Specialist roles cluster around lower medians, reflecting standardized pay structures common in business-facing positions (Glassdoor Economic Research (2024)). Overall, the trend highlights that technical specialization and data infrastructure expertise command higher pay within analytics fields (Bone, Ehlinger, and Stephany (2025)).\n\n\nCode\n#  Copy and clean up data\ndf_remote = df[df[\"Average_Salary\"] &gt; 0].copy()\n\n#  Combine '[None]' and 'Not Remote' into 'Onsite'\ndf_remote[\"REMOTE_GROUP\"] = df_remote[\"REMOTE_GROUP\"].replace({\n    None: \"Onsite\",\n    \"[None]\": \"Onsite\",\n    \"Not Remote\": \"Onsite\"\n})\n\n#  Define custom order for clarity\nremote_order = [\"Onsite\", \"Hybrid Remote\", \"Remote\"]\n\n#  Build polished boxplot\nfig = px.box(\n    df_remote,\n    x=\"REMOTE_GROUP\",\n    y=\"Average_Salary\",\n    color=\"REMOTE_GROUP\",\n    category_orders={\"REMOTE_GROUP\": remote_order},\n    color_discrete_sequence=[\"#187145\", \"#45A274\", \"#79C99E\"],\n    title=\"Salary Distribution by Remote Work Type\",\n    points=\"outliers\",  # show only outliers\n    height=550\n)\n\n#  Aesthetic refinements\nfig.update_layout(\n    title=dict(\n        text=\"&lt;b&gt;Salary Distribution by Remote Work Type&lt;/b&gt;\",\n        x=0.5,\n        xanchor=\"center\",\n        font=dict(size=20)\n    ),\n    xaxis_title=\"&lt;b&gt;Remote Work Type&lt;/b&gt;\",\n    yaxis_title=\"&lt;b&gt;Average Salary (USD)&lt;/b&gt;\",\n    template=\"plotly_white\",\n    width=900,\n    height=550,\n    font=dict(size=14),\n    legend_title_text=\"&lt;b&gt;Remote Work Type&lt;/b&gt;\",\n    legend=dict(\n        orientation=\"v\",\n        yanchor=\"top\",\n        y=0.98,\n        xanchor=\"right\",\n        x=1.12,\n        font=dict(size=12)\n    ),\n    margin=dict(l=60, r=120, t=80, b=100)\n)\n\n#  Axis polish\nfig.update_xaxes(\n    tickangle=0,\n    showgrid=False,\n    tickfont=dict(size=13)\n)\nfig.update_yaxes(showgrid=True, gridcolor=\"lightgray\", zeroline=False)\n\nfig\n\n\n\n\n\n\n\n0.7 Remote Work vs Salary Trends\nThe salary distribution varies noticeably by remote work type. Remote roles show the highest median salaries and a slightly wider interquartile range, indicating greater earning potential and variability among remote positions. Hybrid Remote jobs follow closely, while Onsite roles exhibit lower medians and tighter spreads. This mirrors workforce evidence that remote-capable roles—particularly in analytics and technology—carry a wage premium and higher demand (McKinsey & Company (2022); Lightcast (2024)). The findings align with broader trends emphasizing flexibility and digital collaboration as valued aspects of compensation (Glassdoor Economic Research (2024)).\n\n\nCode\n#  Use the detected skill column\nskills_column = \"COMMON_SKILLS_NAME\"  # replace if another had more data\n\n#  Flatten and clean skills\nskills_flat = [\n    s.strip().replace('\"', '')  # remove quotation marks\n    for sublist in df[skills_column].dropna().astype(str).str.split(',')\n    for s in sublist if s.strip()\n]\n\n#  Count top 15 skills\nskill_counts = pd.DataFrame(\n    Counter(skills_flat).most_common(15),\n    columns=[\"Skill\", \"Count\"]\n)\n\n#  Bar chart\nfig = px.bar(\n    skill_counts,\n    x=\"Skill\",\n    y=\"Count\",\n    title=\"Top 15 Most Frequent Skills\",\n    color_discrete_sequence=[\"#187145\"]\n)\n\n#  Clean layout\nfig.update_layout(\n    title=dict(\n        text=\"&lt;b&gt;Top 15 Most Frequent Skills&lt;/b&gt;\",\n        x=0.5,\n        xanchor=\"center\",\n        font=dict(size=22)\n    ),\n    xaxis_title=\"&lt;b&gt;Skill&lt;/b&gt;\",\n    yaxis_title=\"&lt;b&gt;Number of Job Postings&lt;/b&gt;\",\n    template=\"plotly_white\",\n    width=900,\n    height=550,\n    font=dict(size=14),\n    margin=dict(l=60, r=40, t=80, b=120)\n)\n\n#  Axis styling\nfig.update_xaxes(\n    tickangle=35,\n    tickfont=dict(size=12),\n    showgrid=False,\n    tickvals=list(range(len(skill_counts))),\n    ticktext=[skill.replace(\" (Programming Language)\", \"\") for skill in skill_counts[\"Skill\"]]\n)\nfig.update_yaxes(showgrid=True, gridcolor=\"lightgray\", zeroline=False)\n\nfig\n\n\n\n\n\n\n\n0.8 Top Skills Frequency\nThe top skills highlight a blend of technical, analytical, and interpersonal competencies valued across data-driven roles. Communication and Data Analysis appear most frequently, underscoring the importance of translating insights into business impact. SQL, Python, and Microsoft Excel remain core technical skills, while Project Management and Leadership emphasize strategic coordination (DataCamp (2025); Lightcast (2024)). This distribution reflects the “T-shaped” professional model, where deep technical knowledge is complemented by broad problem-solving and collaboration abilities (Bone, Ehlinger, and Stephany (2025); Mäkelä and Stephany (2024)).\n\n\nCode\n#  Compute correlation matrix\ncorr_cols = [\"Average_Salary\", \"MIN_YEARS_EXPERIENCE\", \"MAX_YEARS_EXPERIENCE\"]\ncorr = df[corr_cols].corr().round(2)\n\n#  Create heatmap\nfig = ff.create_annotated_heatmap(\n    z=corr.values,\n    x=corr.columns.tolist(),\n    y=corr.columns.tolist(),\n    colorscale=\"greens\",\n    showscale=True,\n    annotation_text=corr.values,\n    font_colors=[\"black\"],\n    hoverinfo=\"none\"\n)\n\n#  Layout adjustments\nfig.update_layout(\n    title=dict(\n        text=\"&lt;b&gt;Feature Correlation Matrix&lt;/b&gt;\",\n        x=0.5,\n        xanchor=\"center\",\n        font=dict(size=20)\n    ),\n    template=\"plotly_white\",\n    width=700,\n    height=500,\n    margin=dict(l=100, r=100, t=120, b=80), \n    font=dict(size=14)\n)\n\n#  Axis polish\nfig.update_xaxes(\n    side=\"bottom\",\n    tickangle=35,\n    tickfont=dict(size=12),\n    title_standoff=10\n)\nfig.update_yaxes(\n    tickfont=dict(size=12),\n    autorange=\"reversed\"\n)\n\nfig\n\n\n\n\n\n\n\n0.9 Feature Correlation Analysis Heatmap\nThis correlation analysis evaluates the linear relationships between Average Salary, Minimum Years of Experience, and Maximum Years of Experience. The results show that salary is moderately correlated with both experience metrics — 0.51 with minimum and 0.58 with maximum years of experience. This indicates that as experience increases, compensation tends to rise, though not perfectly linearly (Bureau of Labor Statistics (2024); Mincer (1974)). The strong correlation (1.0) between minimum and maximum experience suggests these variables are closely tied — employers typically define experience ranges that scale together (Lightcast (2024)).\nOverall, the heatmap confirms that experience is a meaningful predictor of salary, supporting its inclusion as a key numerical feature in subsequent regression modeling (Glassdoor Economic Research (2024); McKinsey & Company (2022)).\n\n\n\n\n\nReferences\n\nBone, M., E. G. Ehlinger, and F. Stephany. (2025): “Skills or degree? The rise of skill-based hiring for AI and green jobs,” Technological Forecasting and Social Change, 214, 124042.\n\n\nBureau of Labor Statistics. (2024): “Occupational Employment and Wage Statistics,”https://www.bls.gov/oes/.\n\n\nDataCamp. (2025): “Machine Learning Courses | DataCamp,”\n\n\nGlassdoor Economic Research. (2024): “Data Science Salaries and Market Trends,”\n\n\nLightcast. (2024): “Global Labor Market Insights Report,”https://lightcast.io/resources.\n\n\nMäkelä, E., and F. Stephany. (2024): “Complement or substitute? How AI increases the demand for human skills,” arXiv preprint arXiv:2412.19754,.\n\n\nMcKinsey & Company. (2022): The State of Remote Work: Productivity, Pay, and Preferences,McKinsey Global Institute.\n\n\nMincer, J. (1974): Schooling, Experience, and Earnings, National Bureau of Economic Research."
  },
  {
    "objectID": "ml_methods.html#role-classification-logic",
    "href": "ml_methods.html#role-classification-logic",
    "title": "Machine Learning Methods",
    "section": "3.1 Role Classification Logic",
    "text": "3.1 Role Classification Logic\nThree role categories are identified based on technical skills:\n\nML roles: Require advanced ML/AI skills like TensorFlow, PyTorch, Deep Learning, NLP, Computer Vision\nData Science roles: Require R programming, Python with Statistics, or multiple data science tools (Pandas, NumPy, Scikit-learn)\nBusiness Analytics roles: Require SQL, data analysis, visualization tools (Tableau, Power BI), typically 2+ BA skills\n\nThe analysis examines how these specialized skills impact salary and career opportunities. Machine learning models are used to find patterns that can guide job seekers in choosing which skills to develop."
  },
  {
    "objectID": "ml_methods.html#classification-results-identifying-mldata-science-roles",
    "href": "ml_methods.html#classification-results-identifying-mldata-science-roles",
    "title": "Machine Learning Methods",
    "section": "7.2 Classification Results: Identifying ML/Data Science Roles",
    "text": "7.2 Classification Results: Identifying ML/Data Science Roles\nA Random Forest classifier was used to predict whether a job is an ML/Data Science role based on its skill requirements. The model achieved very strong performance in separating ML/DS roles from Business Analytics and other positions.\nModel Performance:\n\nAccuracy: 99.95% — nearly all ML/DS roles correctly identified\nInsight: ML/DS roles have distinct skill patterns compared to BA and general analyst jobs\nConclusion: Skill-based criteria effectively distinguish ML/DS roles from BA positions\n\nKey Predictive Skills (Feature Importance)\n\nProgramming: Python, R\nML Frameworks: TensorFlow, PyTorch\nStatistical Modeling: Core differentiator for ML/DS\nBA-Oriented Skills: SQL, Tableau, Power BI, Data Analysis (more common in BA roles)\n\nCareer Implications\n\nDistinct skill sets: ML/DS roles require clearly different capabilities than BA roles\nML/DS focus: Programming, modeling, and ML frameworks are the strongest signals\nBA focus: SQL, visualization, and reporting tools dominate BA roles\nCareer development: Building expertise in high-importance ML/DS features directly improves readiness for ML/DS positions\n\nSummary:The Random Forest classifier confirms that ML/DS roles are defined by specialized technical skills, while BA roles emphasize analysis and visualization tools. This distinction provides a clear roadmap for professionals aiming to transition into ML/DS careers."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Business Analytics, Data Science, and Machine Learning Trends",
    "section": "",
    "text": "The global job market continues to transform rapidly as artificial intelligence (AI), automation, and data-driven decision-making redefine how organizations hire and grow talent. This project investigates the intersections of Business Analytics (BA), Data Science (DS), and Machine Learning (ML) roles to uncover which skills, experiences, and qualifications drive the strongest career outcomes in 2025.\nUsing labor market data from Lightcast, combined with additional references from Bureau of Labor Statistics and contemporary academic research, this study explores salary trends, remote work patterns, and skill demand shifts across key industries (Lightcast (2024); Bureau of Labor Statistics (2024); Bone, Ehlinger, and Stephany (2025)).\n\n\n\nThe rapid diffusion of AI and data technologies has elevated the need for specialized technical and analytical expertise across nearly every sector (Mäkelä and Stephany (2024)). Organizations are increasingly adopting skill-based hiring practices over traditional degree-based approaches, emphasizing applied technical proficiency, adaptability, and digital literacy.\nThis analysis helps future professionals — especially graduate students and early-career analysts — understand how specific skills, tools, and experience levels influence employability, compensation, and growth opportunities. By evaluating current job postings, we highlight where the market is heading and how individuals can strategically position themselves in this evolving environment.\n\n\n\nRecent studies emphasize the dual importance of technical and human skills.\n- Bone et al. (2025) found that employers increasingly prioritize demonstrable competencies (e.g., Python, Machine Learning, Cloud Computing) over academic credentials for AI-related jobs.\n- Mäkela & Stephany (2024) observed that AI technologies often complement rather than replace human labor, boosting demand for creativity, communication, and problem-solving alongside data skills.\n- McKinsey (2022) further noted that remote work and digital collaboration are now embedded in the modern work structure, influencing salary and flexibility expectations.\nTogether, these insights form the foundation for our exploratory and predictive modeling analyses, which examine how specific skills translate into measurable labor market value.\n\n\n\nThe website is organized into analytical sections reflecting the end-to-end data science workflow:\n\n\n\n\n\n\n\n\nSection\nFocus\nDeliverable\n\n\n\n\nData Cleaning\nPreprocessing and text normalization using PySpark\nCleaned dataset (lightcast_cleaned.csv)\n\n\nEDA\nVisual and statistical exploration of salary, experience, and skill trends\nInteractive Plotly charts\n\n\nSkill Gap Analysis\nGroup-level skill self-assessment vs. market demand\nHeatmaps and skill frequency analysis\n\n\nMachine Learning Methods\nRegression, clustering, and classification to predict salary and job segmentation\nFeature importance and actionable insights\n\n\nPersonal Career Strategy\nTeam reflections and professional development plans\nIndividualized career roadmaps\n\n\n\nEach component builds toward a comprehensive view of the labor market — connecting data preparation, modeling, and practical application."
  },
  {
    "objectID": "introduction.html#introduction",
    "href": "introduction.html#introduction",
    "title": "Business Analytics, Data Science, and Machine Learning Trends",
    "section": "",
    "text": "The global job market continues to transform rapidly as artificial intelligence (AI), automation, and data-driven decision-making redefine how organizations hire and grow talent. This project investigates the intersections of Business Analytics (BA), Data Science (DS), and Machine Learning (ML) roles to uncover which skills, experiences, and qualifications drive the strongest career outcomes in 2025.\nUsing labor market data from Lightcast, combined with additional references from Bureau of Labor Statistics and contemporary academic research, this study explores salary trends, remote work patterns, and skill demand shifts across key industries (Lightcast (2024); Bureau of Labor Statistics (2024); Bone, Ehlinger, and Stephany (2025))."
  },
  {
    "objectID": "introduction.html#rationale",
    "href": "introduction.html#rationale",
    "title": "Business Analytics, Data Science, and Machine Learning Trends",
    "section": "",
    "text": "The rapid diffusion of AI and data technologies has elevated the need for specialized technical and analytical expertise across nearly every sector (Mäkelä and Stephany (2024)). Organizations are increasingly adopting skill-based hiring practices over traditional degree-based approaches, emphasizing applied technical proficiency, adaptability, and digital literacy.\nThis analysis helps future professionals — especially graduate students and early-career analysts — understand how specific skills, tools, and experience levels influence employability, compensation, and growth opportunities. By evaluating current job postings, we highlight where the market is heading and how individuals can strategically position themselves in this evolving environment."
  },
  {
    "objectID": "introduction.html#literature-insights",
    "href": "introduction.html#literature-insights",
    "title": "Business Analytics, Data Science, and Machine Learning Trends",
    "section": "",
    "text": "Recent studies emphasize the dual importance of technical and human skills.\n- Bone et al. (2025) found that employers increasingly prioritize demonstrable competencies (e.g., Python, Machine Learning, Cloud Computing) over academic credentials for AI-related jobs.\n- Mäkela & Stephany (2024) observed that AI technologies often complement rather than replace human labor, boosting demand for creativity, communication, and problem-solving alongside data skills.\n- McKinsey (2022) further noted that remote work and digital collaboration are now embedded in the modern work structure, influencing salary and flexibility expectations.\nTogether, these insights form the foundation for our exploratory and predictive modeling analyses, which examine how specific skills translate into measurable labor market value."
  },
  {
    "objectID": "introduction.html#project-structure",
    "href": "introduction.html#project-structure",
    "title": "Business Analytics, Data Science, and Machine Learning Trends",
    "section": "",
    "text": "The website is organized into analytical sections reflecting the end-to-end data science workflow:\n\n\n\n\n\n\n\n\nSection\nFocus\nDeliverable\n\n\n\n\nData Cleaning\nPreprocessing and text normalization using PySpark\nCleaned dataset (lightcast_cleaned.csv)\n\n\nEDA\nVisual and statistical exploration of salary, experience, and skill trends\nInteractive Plotly charts\n\n\nSkill Gap Analysis\nGroup-level skill self-assessment vs. market demand\nHeatmaps and skill frequency analysis\n\n\nMachine Learning Methods\nRegression, clustering, and classification to predict salary and job segmentation\nFeature importance and actionable insights\n\n\nPersonal Career Strategy\nTeam reflections and professional development plans\nIndividualized career roadmaps\n\n\n\nEach component builds toward a comprehensive view of the labor market — connecting data preparation, modeling, and practical application."
  },
  {
    "objectID": "ml_methods.html#kmeans-clustering-based-on-skills",
    "href": "ml_methods.html#kmeans-clustering-based-on-skills",
    "title": "Machine Learning Methods",
    "section": "5.1 KMeans Clustering Based on Skills",
    "text": "5.1 KMeans Clustering Based on Skills\nThe first machine learning approach uses KMeans clustering to discover natural groupings in the job market. This unsupervised technique groups jobs with similar skill profiles together, without using salary information. The goal is to see if jobs naturally segment into distinct categories based on their requirements.\n\n\nCode\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_squared_error, r2_score, accuracy_score, f1_score, confusion_matrix, classification_report\n\n# Prepare features for clustering using skills and other features\nskill_feature_cols = [col for col in df_modeling.columns if col.startswith('has_')]\nprint(f\"Available skill features: {len(skill_feature_cols)}\")\n\n# Base clustering features\nclustering_features = skill_feature_cols + ['experience_years', 'is_remote']\n\n# Encode ONET and NAICS6.\nle_onet = LabelEncoder()\ndf_modeling['onet_encoded'] = le_onet.fit_transform(df_final['ONET'].fillna('Unknown'))\nclustering_features.append('onet_encoded')\n\nle_naics = LabelEncoder()\ndf_modeling['naics_encoded'] = le_naics.fit_transform(df_final['NAICS6'].fillna('Unknown'))\nclustering_features.append('naics_encoded')\n\n# Prepare clustering data\nX_cluster = df_modeling[clustering_features].fillna(0)\n\n# Scale features\nscaler_cluster = StandardScaler()\nX_cluster_scaled = scaler_cluster.fit_transform(X_cluster)\n\n# KMeans clustering\nkmeans = KMeans(n_clusters=6, random_state=42, n_init=10)\nclusters = kmeans.fit_predict(X_cluster_scaled)\ndf_modeling['cluster'] = clusters\n\n# print(\"Skills based clustering completed\")\n# print(\"Cluster centers:\")\n# for i, center in enumerate(kmeans.cluster_centers_):\n#     print(f\"Cluster {i}: {center}\")\n\n\nAvailable skill features: 25\n\n\nThe clustering model groups similar jobs together using skill patterns, experience requirements, and job characteristics. The algorithm assigns each job to one of 6 clusters. Now the characteristics of each cluster can be examined to understand what makes them distinct.\n\n\nCode\n# Analyze clustering.\ncluster_summary = df_modeling.groupby('cluster').agg({\n    'SALARY': ['count', 'mean'],\n    'is_ml_role': 'mean',\n    'is_ds_role': 'mean',\n    'is_ba_role': 'mean',\n    'is_remote': 'mean',\n    'experience_years': 'mean'\n}).round(2)\n\ncluster_summary.columns = ['count', 'avg_salary', 'ml_role_pct', 'ds_role_pct', 'ba_role_pct',\n                        'remote_percentage', 'avg_experience']\ncluster_summary = cluster_summary.reset_index()\n\n# Compute combined BA/ML/DS percentage on-the-fly\n# A job has BA/ML/DS if it has any of the three role types\ncluster_summary['ml_ds_ba_combined_pct'] = cluster_summary.apply(\n    lambda row: ((df_modeling[df_modeling['cluster'] == row['cluster']][['is_ml_role', 'is_ds_role', 'is_ba_role']].sum(axis=1) &gt; 0).mean()),\n    axis=1\n).round(2)\n\nprint(\"Skills based Cluster Summary:\")\nprint(cluster_summary)\n\n# Visualize cluster characteristics.\nfig = make_subplots(\n    rows=2, cols=3,\n    subplot_titles=('Cluster Size', 'Average Salary', 'BA/ML/DS Role %',\n                'Remote Work %', 'Avg Experience', 'Salary Distribution'),\n    specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}, {\"type\": \"bar\"}],\n        [{\"type\": \"bar\"}, {\"type\": \"bar\"}, {\"type\": \"scatter\"}]]\n)\n\nfig.add_trace(go.Bar(x=cluster_summary['cluster'], y=cluster_summary['count'], name=\"Count\"), row=1, col=1)\nfig.add_trace(go.Bar(x=cluster_summary['cluster'], y=cluster_summary['avg_salary'], name=\"Avg Salary\"), row=1, col=2)\nfig.add_trace(go.Bar(x=cluster_summary['cluster'], y=cluster_summary['ml_role_pct'], name=\"ML %\"), row=1, col=3)\nfig.add_trace(go.Bar(x=cluster_summary['cluster'], y=cluster_summary['ds_role_pct'], name=\"DS %\"), row=1, col=3)\nfig.add_trace(go.Bar(x=cluster_summary['cluster'], y=cluster_summary['ba_role_pct'], name=\"BA %\"), row=1, col=3)\nfig.add_trace(go.Bar(x=cluster_summary['cluster'], y=cluster_summary['remote_percentage'], name=\"Remote %\"), row=2, col=1)\nfig.add_trace(go.Bar(x=cluster_summary['cluster'], y=cluster_summary['avg_experience'], name=\"Experience\"), row=2, col=2)\n\n# Salary distribution by cluster.\nfig.add_trace(\n    go.Scatter(\n        x=df_modeling['cluster'],\n        y=df_modeling['SALARY'],\n        mode='markers',\n        opacity=0.6,\n        name=\"Jobs\"\n    ),\n    row=2, col=3\n)\n\nfig.update_layout(\n    height=650,\n    showlegend=False,\n    template=\"plotly_white\",\n    title={\n        'text': \"Skills-Based KMeans Clustering Results\",\n        'y': 0.98,\n        'x': 0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'\n    },\n    margin=dict(t=80)\n)\nfig.show()\n\n\nSkills based Cluster Summary:\n   cluster  count  avg_salary  ml_role_pct  ds_role_pct  ba_role_pct  \\\n0        0    583   139707.42         0.60         0.26         0.70   \n1        1  10189   144796.54         0.14         0.00         0.04   \n2        2  13313   100969.83         0.01         0.01         0.28   \n3        3   6573   108557.35         0.17         0.39         0.95   \n4        4     77   140001.35         1.00         0.32         0.31   \n5        5     73   117793.86         0.55         0.32         0.93   \n\n   remote_percentage  avg_experience  ml_ds_ba_combined_pct  \n0               0.44            4.45                   0.90  \n1               0.25            7.80                   0.17  \n2               0.39            2.00                   0.29  \n3               0.48            3.27                   0.99  \n4               0.34            4.23                   1.00  \n5               0.56            3.01                   0.96  \n\n\n        \n        \n        \n\n\n                                                    \n\n\n\n5.1.1 Insights from KMeans Clustering\nThe clustering analysis grouped jobs based on their skill requirements and characteristics. The analysis identified 6 distinct job clusters, each with different salary levels, remote work availability, and skill profiles.\nKey Findings:\n\nBusiness Analytics dominates: 10,831 BA roles vs. 3,226 ML and 2,877 DS\nCluster 0 (583 jobs, $140K): High-skill hybrid (60% ML, 26% DS, 70% BA)\nCluster 1 (10,189 jobs, $145K): Mostly general tech, only 17% BA/DS/ML, highest pay\nCluster 2 (13,313 jobs, $101K): Entry-level, lowest experience (2 yrs), BA-focused (28%)\nCluster 3 (6,573 jobs, $109K): BA-heavy (95%) with DS overlap (39%)\nCluster 4 (77 jobs, $140K): Pure ML specialists (100% ML), niche but high-paying\nCluster 5 (73 jobs, $118K): Hybrid roles (96% BA/DS/ML), most remote-friendly (56%)\nRemote work: 25%–56% across clusters\nExperience: 2.0–7.8 years, showing clear career progression\n\nCareer Implications:\n\nMost opportunities: Business Analytics (SQL, Tableau, Power BI, visualization)\nHighest pay + volume: Cluster 1 ($145K, 10K+ jobs) — general tech roles\nEntry path: Cluster 2 ($101K, 13K jobs) — BA-focused, lowest experience needed\nBA-focused growth: Cluster 3 ($109K) — strong BA demand with DS hybrid edge\nSpecialist track: Cluster 4 ($140K) — pure ML, fewer jobs but high pay\nHybrid advantage: Cluster 0 ($140K) and Cluster 5 ($118K, 56% remote) — multi-skill roles with flexibility"
  },
  {
    "objectID": "ml_methods.html#multiple-regression",
    "href": "ml_methods.html#multiple-regression",
    "title": "Machine Learning Methods",
    "section": "6.1 Multiple Regression",
    "text": "6.1 Multiple Regression\nThe second approach uses supervised learning to predict salary based on skills and experience. Two regression models are trained: Multiple Linear Regression and Random Forest. This analysis identifies which skills and factors most strongly influence compensation.\n\n\nCode\n# Identify regression features.\n# Focus on skills (not role labels) to understand how skills directly affect salary\nregression_features = skill_feature_cols + ['experience_years', 'is_remote']\n\n# Prepare regression data using salary as the target variable\nX_reg = df_modeling[regression_features].fillna(0)\ny_reg = df_modeling['SALARY']\n\nX_train, X_test, y_train, y_test = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n\nprint(f\"Training set size: {len(X_train):,}\")\nprint(f\"Test set size: {len(X_test):,}\")\n\n# Scale features\nscaler_reg = StandardScaler()\nX_train_scaled = scaler_reg.fit_transform(X_train)\nX_test_scaled = scaler_reg.transform(X_test)\n\n# Multiple Linear Regression\nlr = LinearRegression()\nlr.fit(X_train_scaled, y_train)\n\n# Random Forest Regression\nrf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\nrf_reg.fit(X_train_scaled, y_train)\n\nprint(\"Skills based regression models training completed\")\n\n\nTraining set size: 24,646\nTest set size: 6,162\nSkills based regression models training completed\n\n\nBoth models are trained on 80% of the data and will be evaluated on the remaining 20% test set. The Random Forest model can capture non-linear relationships and interactions between skills, while Multiple Linear Regression provides a baseline for comparison.\n\n\nCode\n# Evaluate regression models\n# Multiple Linear Regression predictions\ny_pred_lr = lr.predict(X_test_scaled)\nrmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\nr2_lr = r2_score(y_test, y_pred_lr)\n\n# Random Forest predictions\ny_pred_rf = rf_reg.predict(X_test_scaled)\nrmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\nr2_rf = r2_score(y_test, y_pred_rf)\n\nprint(\"Skills-based Regression Model Performance:\")\nprint(f\"Multiple Linear Regression - RMSE: ${rmse_lr:,.2f}, R²: {r2_lr:.4f}\")\nprint(f\"Random Forest - RMSE: ${rmse_rf:,.2f}, R²: {r2_rf:.4f}\")\n\n# Feature importance for Random Forest\n# Only use features that actually exist in the model\nactual_feature_names = [col for col in regression_features if col in X_train.columns]\nimportances = rf_reg.feature_importances_\n\n# Visualize feature importance\nfig = px.bar(x=actual_feature_names, y=importances,\n            title=\"Skills Impact on Salary (Random Forest Feature Importance)\",\n            labels={'x': 'Features', 'y': 'Importance'})\nfig.update_layout(template=\"plotly_white\", xaxis_tickangle=-45)\nfig.show()\n\n# Top skills by salary impact\nskill_importance = list(zip(actual_feature_names, importances))\nskill_importance.sort(key=lambda x: x[1], reverse=True)\nprint(\"\\nTop skills by salary impact:\")\nfor skill, importance in skill_importance[:10]:\n    print(f\"{skill}: {importance:.4f}\")\n\n\nSkills-based Regression Model Performance:\nMultiple Linear Regression - RMSE: $37,899.01, R²: 0.2780\nRandom Forest - RMSE: $32,558.54, R²: 0.4672\n\nTop skills by salary impact:\nexperience_years: 0.4932\nis_remote: 0.0728\nhas_data_analysis: 0.0426\nhas_tableau_business_intelligence_software: 0.0372\nhas_amazon_web_services: 0.0361\nhas_sql_programming_language: 0.0350\nhas_statistics: 0.0302\nhas_python_programming_language: 0.0300\nhas_machine_learning: 0.0265\nhas_data_science: 0.0261\n\n\n                                                    \n\n\n\n6.1.1 Regression Analysis: What drives salary?\nPrediction models were built to understand how skills influence salary. The Random Forest model achieved R2 of 0.47 compared to 0.28 for Multiple Linear Regression, showing that skill-salary relationships are complex.\nModel Performance:\n\nRandom Forest: R² = 0.47 (explains 47% of salary variation), RMSE = $32,559\nMultiple Linear Regression: R² = 0.28\nInsight: Skills alone do not fully explain salary — other factors also matter.\n\nKey Salary Drivers (Feature Importance):\n\nExperience (0.49): Largest factor, nearly half of salary variation\nRemote work (0.07): Flexibility influences pay differences\nData Analysis (0.04): Core analytical capability\nTableau (0.04): Visualization and BI tool\nAWS (0.04): Cloud computing platform\nSQL (0.04): Database querying and manipulation\nStatistics (0.03): Analytical foundation\nPython (0.03): Programming language\n\nCareer Implications:\n\nExperience is critical — the strongest driver of salary.\nRemote work adds value — flexibility can boost compensation.\nSkill combinations matter — technical, analytical, and cloud skills together shape salary outcomes.\n\nSummary: Salary is not determined by skills alone. Experience and work flexibility are key, while technical skills provide additional differentiation."
  },
  {
    "objectID": "ml_methods.html#classification-to-identify-bamlds-roles",
    "href": "ml_methods.html#classification-to-identify-bamlds-roles",
    "title": "Machine Learning Methods",
    "section": "6.2 Classification to Identify BA/ML/DS Roles",
    "text": "6.2 Classification to Identify BA/ML/DS Roles\nAlthough the project required only one of the supervised learning models. This analysis also explores the classification to distinguish ML/Data Science roles from Business Analytics and other positions. A Random Forest Classifier is trained to predict whether a job is an ML/DS role based on its skill requirements. This analysis reveals which skills are the strongest “signature” indicators that distinguish ML/DS positions from BA roles.\n\n\nCode\n# Prepare features for classification.\nclassification_features = skill_feature_cols + ['experience_years', 'is_remote']\n\n# Prepare classification data\nX_clf = df_modeling[classification_features].fillna(0)\n# Target: ML/DS roles (computed from is_ml_role OR is_ds_role)\ny_clf = ((df_modeling['is_ml_role'] == 1) | (df_modeling['is_ds_role'] == 1)).astype(int)\n\n# Train/test split for classification\nX_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X_clf, y_clf, test_size=0.2, random_state=42)\n\n# Scale features\nscaler_clf = StandardScaler()\nX_train_clf_scaled = scaler_clf.fit_transform(X_train_clf)\nX_test_clf_scaled = scaler_clf.transform(X_test_clf)\n\n# Random Forest Classification\nrf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_clf.fit(X_train_clf_scaled, y_train_clf)\n\nprint(\"Skills-based classification model trained successfully!\")\n\n\nSkills-based classification model trained successfully!\n\n\nThe classifier learns patterns that distinguish ML/DS roles from BA and other positions based on their skill profiles. The model is now evaluated to see how accurately it can identify these specialized ML/DS roles versus the more common BA positions.\n\n\nCode\n# Random Forest predictions\ny_pred_rf_clf = rf_clf.predict(X_test_clf_scaled)\naccuracy_rf = accuracy_score(y_test_clf, y_pred_rf_clf)\nf1_rf = f1_score(y_test_clf, y_pred_rf_clf)\n\nprint(\"Skills based Classification Model Performance:\")\nprint(f\"Random Forest - Accuracy: {accuracy_rf:.4f}, F1 Score: {f1_rf:.4f}\")\n\n# Confusion Matrix for Random Forest\ncm = confusion_matrix(y_test_clf, y_pred_rf_clf)\n\n# Visualize confusion matrix\nfig = px.imshow(cm, text_auto=True, aspect=\"auto\",\n                title=\"Confusion Matrix - ML/DS Role Classification\",\n                labels=dict(x=\"Predicted\", y=\"Actual\"),\n                color_continuous_scale=\"Blues\")\n\nfig.update_layout(template=\"plotly_white\")\nfig.update_xaxes(tickvals=[0,1], ticktext=['Not ML/DS', 'ML/DS'])\nfig.update_yaxes(tickvals=[0,1], ticktext=['Not ML/DS', 'ML/DS'])\nfig.show()\n\nprint(\"Classification Report:\")\nprint(classification_report(y_test_clf, y_pred_rf_clf))\n\n# Only use features that actually exist in the classification model\nclf_actual_feature_names = [col for col in classification_features if col in X_train_clf.columns]\nclf_importances = rf_clf.feature_importances_\n\n# Visualize classification feature importance\nfig = px.bar(x=clf_actual_feature_names, y=clf_importances,\n            title=\"Skills Impact on ML/Data Science Role Classification\",\n            labels={'x': 'Features', 'y': 'Importance'})\nfig.update_layout(template=\"plotly_white\", xaxis_tickangle=-45)\nfig.show()\n\n\nSkills based Classification Model Performance:\nRandom Forest - Accuracy: 0.9995, F1 Score: 0.9986\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00      5082\n           1       1.00      1.00      1.00      1080\n\n    accuracy                           1.00      6162\n   macro avg       1.00      1.00      1.00      6162\nweighted avg       1.00      1.00      1.00      6162\n\n\n\n                                                    \n\n\n                                                    \n\n\n\n6.2.1 Classification Results: Identifying ML/Data Science Roles\nA Random Forest classifier was used to predict whether a job is an ML/Data Science role based on its skill requirements. The model achieved very strong performance in separating ML/DS roles from Business Analytics and other positions.\nModel Performance:\n\nAccuracy: 99.95% — nearly all ML/DS roles correctly identified\nInsight: ML/DS roles have distinct skill patterns compared to BA and general analyst jobs\nConclusion: Skill-based criteria effectively distinguish ML/DS roles from BA positions\n\nKey Predictive Skills (Feature Importance)\n\nProgramming: Python, R\nML Frameworks: TensorFlow, PyTorch\nStatistical Modeling: Core differentiator for ML/DS\nBA-Oriented Skills: SQL, Tableau, Power BI, Data Analysis (more common in BA roles)\n\nCareer Implications\n\nDistinct skill sets: ML/DS roles require clearly different capabilities than BA roles\nML/DS focus: Programming, modeling, and ML frameworks are the strongest signals\nBA focus: SQL, visualization, and reporting tools dominate BA roles\nCareer development: Building expertise in high-importance ML/DS features directly improves readiness for ML/DS positions\n\nSummary:The Random Forest classifier confirms that ML/DS roles are defined by specialized technical skills, while BA roles emphasize analysis and visualization tools. This distinction provides a clear roadmap for professionals aiming to transition into ML/DS careers."
  },
  {
    "objectID": "data_analysis.html",
    "href": "data_analysis.html",
    "title": "Data Cleaning & Rationale",
    "section": "",
    "text": "In this section, we outline key decisions made during the data preprocessing phase, focusing on column relevance, code redundancy, and the impact on downstream analysis.\n\n\n\nAfter reviewing the dataset, we identified several columns that were either:\n\nIrrelevant to our analysis goals (e.g., internal IDs, timestamps not tied to labor trends)\nRedundant due to duplication or overlapping information\n\n\n\n\nrecord_id, submission_timestamp: Metadata not used in analysis\nnaics_code_2017, naics_code_2022: Multiple versions of the same classification system\nsoc_code_2010, soc_code_2018: Legacy codes that overlap with updated versions\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNAICS (North American Industry Classification System) and SOC (Standard Occupational Classification) codes are updated periodically. Including multiple versions introduces: - Redundancy - Confusion in grouping industries or occupations - Risk of double-counting or misalignment in trend analysis\n\n\nWe retained only the most recent version of each code to ensure consistency and relevance to 2024 labor market trends.\n\n\n\n\nCleaning the dataset in this way improves our analysis by:\n\nReducing noise: Fewer columns means clearer signals\nImproving interpretability: Analysts and readers can focus on current classifications\nEnhancing visualizations: Charts and tables are easier to read and more meaningful\nEnsuring consistency: Aligns with external sources like Lightcast and BLS data\n\n\n\n\n\nWith a cleaner dataset, we’re now ready to explore key workforce themes—such as AI-driven job growth, salary disparities, and gender-based employment patterns—using reliable, streamlined data."
  },
  {
    "objectID": "data_analysis.html#identifying-irrelevant-or-redundant-columns",
    "href": "data_analysis.html#identifying-irrelevant-or-redundant-columns",
    "title": "Data Cleaning & Rationale",
    "section": "",
    "text": "After reviewing the dataset, we identified several columns that were either:\n\nIrrelevant to our analysis goals (e.g., internal IDs, timestamps not tied to labor trends)\nRedundant due to duplication or overlapping information\n\n\n\n\nrecord_id, submission_timestamp: Metadata not used in analysis\nnaics_code_2017, naics_code_2022: Multiple versions of the same classification system\nsoc_code_2010, soc_code_2018: Legacy codes that overlap with updated versions"
  },
  {
    "objectID": "data_analysis.html#why-remove-multiple-versions-of-naicssoc-codes",
    "href": "data_analysis.html#why-remove-multiple-versions-of-naicssoc-codes",
    "title": "Data Cleaning & Rationale",
    "section": "",
    "text": "Note\n\n\n\nNAICS (North American Industry Classification System) and SOC (Standard Occupational Classification) codes are updated periodically. Including multiple versions introduces: - Redundancy - Confusion in grouping industries or occupations - Risk of double-counting or misalignment in trend analysis\n\n\nWe retained only the most recent version of each code to ensure consistency and relevance to 2024 labor market trends."
  },
  {
    "objectID": "data_analysis.html#how-this-improves-analysis",
    "href": "data_analysis.html#how-this-improves-analysis",
    "title": "Data Cleaning & Rationale",
    "section": "",
    "text": "Cleaning the dataset in this way improves our analysis by:\n\nReducing noise: Fewer columns means clearer signals\nImproving interpretability: Analysts and readers can focus on current classifications\nEnhancing visualizations: Charts and tables are easier to read and more meaningful\nEnsuring consistency: Aligns with external sources like Lightcast and BLS data"
  },
  {
    "objectID": "data_analysis.html#next-steps",
    "href": "data_analysis.html#next-steps",
    "title": "Data Cleaning & Rationale",
    "section": "",
    "text": "With a cleaner dataset, we’re now ready to explore key workforce themes—such as AI-driven job growth, salary disparities, and gender-based employment patterns—using reliable, streamlined data."
  },
  {
    "objectID": "index.html#team-contributions",
    "href": "index.html#team-contributions",
    "title": "Business Analytics, Data Science and Machine Learning Trends",
    "section": "1.3 Team Contributions",
    "text": "1.3 Team Contributions\nThis project was collaboratively completed by Group 13 as part of Boston University’s Applied Business Analytics program.\nEach member contributed to specific modules that collectively formed the foundation of this website and analysis:\n\nAnu Sharma – Led Module 1 (Setup) and Module 4 (EDA and Dashboarding), handling initial data preparation, model setup, and exploratory analysis design. Contributed heavily to the Machine Learning Methods and Data Cleaning sections.\nCindy Guzman – Owned Module 3 (Datalake and Warehouses), focusing on structured data pipelines and storage logic. Led the Exploratory Data Analysis (EDA) narrative, correlation heatmaps, and dashboard development.\nGavin Boss – Completed Module 2 (SQL and Data Modelling), designing relational data models and integrating cleaned data for analysis. Also authored the Skill Gap Analysis section and supported website publishing.\nAll Members – Collaboratively completed Module 5 (Export and Submission) and Module 6 (PPT and Report), including final integration, presentation design, and site documentation.\n\nTogether, the team ensured analytical rigor, visual clarity, and cohesive storytelling across all pages: Introduction, Data Cleaning, EDA, Machine Learning Methods, Skill Gap Analysis, and Personal Career Strategy Plan."
  }
]
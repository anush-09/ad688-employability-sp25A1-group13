[
  {
    "objectID": "ml_methods.html",
    "href": "ml_methods.html",
    "title": "Business Analytics, Data Science and Machine Learning Trends",
    "section": "",
    "text": "This analysis explores trends in Business Analytics, Data Science, and Machine Learning job postings by focusing on the skills required for these roles. We analyze how different skill combinations impact salary, remote work opportunities, and career paths."
  },
  {
    "objectID": "ml_methods.html#important-skills-columns",
    "href": "ml_methods.html#important-skills-columns",
    "title": "Business Analytics, Data Science and Machine Learning Trends",
    "section": "2.1 IMPORTANT Skills columns",
    "text": "2.1 IMPORTANT Skills columns\nWe identified that ‘SKILLS_NAME, ’SOFTWARE_SKILLS_NAME’ and ‘SPECIALIZED_SKILLS_NAME’ columns are most relevant to the analysis."
  },
  {
    "objectID": "ml_methods.html#heuristics-employed-for-mlds-roles",
    "href": "ml_methods.html#heuristics-employed-for-mlds-roles",
    "title": "Business Analytics, Data Science and Machine Learning Trends",
    "section": "3.1 Heuristics employed for ML/DS roles",
    "text": "3.1 Heuristics employed for ML/DS roles\nWe identified ML and Data Science roles using specific technical skills listed in the dataset. ML roles need skills like TensorFlow, PyTorch, Deep Learning, etc. Data Science roles typically require R programming, Python with Statistics, or multiple data analysis tools.\nThe goal is to see how these specialized skills impact salary and career opportunities. We’ll use machine learning models to find patterns that can guide job seekers in choosing which skills to develop."
  },
  {
    "objectID": "ml_methods.html#insights-from-kmeans-clustering",
    "href": "ml_methods.html#insights-from-kmeans-clustering",
    "title": "Business Analytics, Data Science and Machine Learning Trends",
    "section": "5.1 Insights from KMeans Clustering",
    "text": "5.1 Insights from KMeans Clustering\nThe clustering analysis grouped jobs based on their skill requirements and characteristics. We found 6 distinct job clusters, each with different salary levels, remote work availability, and skill profiles.\nKey Findings:\n\nCluster 4 shows higher concentrations of ML/DS roles with corresponding higher average salaries\nRemote work availability varies significantly across clusters, suggesting that certain skill combinations are more compatible with remote positions.\nExperience requirements differ by cluster, indicating distinct career progression paths.\n\nTakeaways for Job Seekers:\n\nThe specialized ML/DS cluster (Cluster 4) offers good pay but fewer opportunities.\nTarget skills that appear in higher-paying clusters to maximize salary potential\nIf remote work is important, target skills common in Cluster(5)."
  },
  {
    "objectID": "ml_methods.html#regression-analysis-what-drives-salary",
    "href": "ml_methods.html#regression-analysis-what-drives-salary",
    "title": "Business Analytics, Data Science and Machine Learning Trends",
    "section": "6.1 Regression Analysis: What drives salary?",
    "text": "6.1 Regression Analysis: What drives salary?\nWe built the prediction models to understand how skills influence salary. The Random Forest model achieved R2 of 0.47 compared to 0.28 for Linear Regression, showing that skill-salary relationships are complex.\nModel Performance: Random Forest R2 of 0.07 means it explains 47% of salary variation. RMSE of 32514 shows typical prediction error It shows skill alone don’t fully determine salary - other factors matter too\nFeature Importance Results: The chart shows which skills have the strongest impact on salary predictions. Top Salary Drivers(by importance): 1. Experience years(0.49) - the biggest factor, almost half of salary determination 2. Remote work(0.07) - remote positions tend to pay differently 3. Data Analysis skill(0.04) - core analytical capability 4. Tableau(0.04) - visualization and BI tool 5. AWS(0.04) - cloud computing platform 6. SQL(0.04) - database querying 7. Python(0.03) - programming language\nImplications for Career Development: Experience matters - nearly 50% of salary impact comes from years of computer Remote works capability add salary premium Combination of skills drives salary differences Experience(years) and remote work flexibility influence compensation"
  },
  {
    "objectID": "ml_methods.html#clasification-results-identifying-mldata-science-roles",
    "href": "ml_methods.html#clasification-results-identifying-mldata-science-roles",
    "title": "Business Analytics, Data Science and Machine Learning Trends",
    "section": "7.1 Clasification Results: Identifying ML/Data Science Roles",
    "text": "7.1 Clasification Results: Identifying ML/Data Science Roles\nThe classification model predicts whether a job is an ML/Data Science role based on its skill requirements. The Random Forest Classifier achieved stong performance in distinguishing these specialized roles from analyst positions.\nModel Performance Interpretation: -Accuracy shows the model correctly identified all roles -It suggests ML/DS roles have very distinct skill patterns compared to other data jobs -This high accuracy indicates our skill based criteria effectively separates these roles\nFeature Importance This chart shows which skills are strongest predictors of ML/DS classification. Skills with higher bars are the “signature” skills that clearly distinguish ML/DS roles from analyst positions.\nActionable Insights - The perfect accuracy shows DL/MS roles require distinctly different skill sets - Focus on the top features to signal ML/DS capabilities to employers - These specialized skills are what separate advance roles from general analytics - Building expertise in high importance features directly increases ML/DS role readiness"
  },
  {
    "objectID": "ml_methods.html#summary-of-findings",
    "href": "ml_methods.html#summary-of-findings",
    "title": "Business Analytics, Data Science and Machine Learning Trends",
    "section": "9.1 Summary of Findings",
    "text": "9.1 Summary of Findings\nOur analysis of business analytics, data science and machine learning job postings reveals several important patterns:\n\nSkill-Based Job Segmentation: Jobs cluster into 6 distinct groups. Cluster 4 (pure ML/DS) pays $140K with only 77 positions, while Cluster 1 (10,189 jobs) pays $145K with mixed roles. Remote work availability varies from 25% to 56% across clusters.\nSalary Drivers: Experience dominates (49% importance) followed by remote work capability (7%). Technical skills like Tableau, AWS, SQL and Python each contribute 3-4%. The R² of 0.47 shows skills explain about half of salary variation.\nRole Differentiation: ML/DS roles have distinct skill patterns, achieving 100% classification accuracy. This indicates these specialized positions require clearly different capabilities than analyst roles."
  },
  {
    "objectID": "ml_methods.html#recommendations-for-job-seekers",
    "href": "ml_methods.html#recommendations-for-job-seekers",
    "title": "Business Analytics, Data Science and Machine Learning Trends",
    "section": "9.2 Recommendations for Job Seekers",
    "text": "9.2 Recommendations for Job Seekers\nFor Career Advancement: - Gain experience - it’s the single biggest salary driver (49% importance) - Develop remote work capabilities - adds 7% to salary potential - Learn practical tools: Tableau, AWS, SQL are each worth 3-4% salary impact - Not only ML/DS titles can be considered as Cluster 1 (non-ML) pays $145K vs Cluster 4 (pure ML) at $140K\nFor Transitioning to ML/Data Science: - The 100% classification accuracy shows these roles need very specific skill combinations - Focus on the specialized skills shown in the classification importance chart - Note: ML/DS specialization has fewer opportunities (Cluster 4 has only 77 jobs)\nFor Maximizing Opportunities: - For remote work: Target Cluster 5 skills (56% remote, 63% ML/DS roles) - For job volume: Cluster 1 has most opportunities (10,189 jobs) at highest pay ($145K) - For specialization: Cluster 4 is pure ML/DS but limited opportunities (77 jobs)"
  },
  {
    "objectID": "ml_methods.html#limitations-and-considerations",
    "href": "ml_methods.html#limitations-and-considerations",
    "title": "Business Analytics, Data Science and Machine Learning Trends",
    "section": "9.3 Limitations and Considerations",
    "text": "9.3 Limitations and Considerations\n\nThe analysis is based on job posting data which may not reflect actual hiring outcomes\nSkill requirements in job posts may differ from day-to-day job responsibilities\nMarket conditions and geographic factors also influence salaries beyond just skills\nThe models identify patterns but don’t capture all nuances of career success"
  },
  {
    "objectID": "skill_gap_analysis.html",
    "href": "skill_gap_analysis.html",
    "title": "Skill Gap Analysis",
    "section": "",
    "text": "1 Team Dataframe\nOur team conducted an overview of our current technical skills when it comes to Business Analytics and our skill levels. The goal was to understand where the group’s strengths and weaknesses were, and understand how the data that was collected could relate to the data that was observed in the research conducted. This was used to identify strengths and weaknesses for the group while also identifying potential gaps within the group.\n\n\nCode\nimport pandas as pd\n\n# Extended skills data\nskills_data = {\n    \"Name\": [\"Anu\", \"Cindy\", \"Gavin\"],\n    \"Data Analysis\": [5, 4, 4],\n    \"Python\": [3, 3, 3],\n    \"SQL\": [5, 3, 2],\n    \"Machine Learning\": [2, 2, 2],\n    \"Cloud Computing\": [3, 3, 2],\n    \"Power BI\": [3, 5, 3],\n    \n}\n\n# Create DataFrame and set index\ndf_skills = pd.DataFrame(skills_data).set_index(\"Name\")\n\n# Ensure all columns are numeric\ndf_skills = df_skills.apply(pd.to_numeric)\n\n# Add the Average row\ndf_skills[\"Average\"] = df_skills.mean(axis=1)\n\n# Style with borders and centered text\ndf_styled = df_skills.style.set_table_styles([\n    {'selector': 'th, td', 'props': [('border', '1px solid black'),\n                                     ('text-align', 'center')]}\n])\n\n# df_styled\n\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nsns.heatmap(df_skills, annot=True, cmap=\"coolwarm\", linewidths=0.5)\nplt.title(\"Team Skill Levels Heatmap\")\nplt.xlabel(\"Technical Skills\", fontsize=14)\nplt.ylabel(\"Team Members\", fontsize=14)\nplt.xticks(rotation=30, fontsize=8)  \nplt.yticks(rotation=90, fontsize=8)\nannot_kws={\"fontsize\":8}\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n2 What the Heatmap Shows\nWhen looking at the heatmap it shows that our team still feels relatively new to these technical skills and highlighted in blue are the most notable areas for growth. One person in our group is a Business Analyst while the other two do not use these tools on a daily basis and due to this lack of exposure likely contribute to a lack of technical skill observed in the heatmap. Increased exposure and experience outside of an educational setting could contribute to increased technical skills within the group.\nData Analysis represents the strongest skill that our group presents, while this isn’t a program it was noted as one of the top skills that impacted salary, alongside experience and being remote. Regardless of the type of data analysis experience, having the ability to analyze data is more vital according to the data represented in the ML Model section of this website. Having a core analytical ability puts our group ahead of most canidates that have vast python experience depending on the types of jobs that they are looking to apply for, and adding the other analytical tools such as Python and PowerBI only empower our group further in the ability to find higher paying jobs.\nPowerBI is our second strongest skill as a group, and while this isn’t a program that is used within the program it was also ranked number 10 when looking at the number times it was mentioned in job descriptions. As can be seen lower on this page it was mentioned 494 times. This is still a strong skill as it can be used in combination with other Microsoft suite programs such as Excel which had a count of 1494 in job descriptions. These in combination with eachother can aid in qualifying for a greater number of jobs. Excel was excluded from the list of skills to conform to size, but is without question also a valuable technical skill.\nPython is also a strong skill across the team and this is likely in large part due to the exposure that has occured during this course, while no one in the group are experts in Python our technical skill levels indicate that we have a foundational understanding of the programming language and are able to navigate the complexities and manipulate data in order to meet organizational goals and objectives required for data analysis, automation, and model development that was observed during this course.\nCloud Computing was one of the areas where we struggled as a group and this was in large part due to a lack of exposure and could likely increase in environments where this was utilized similar to our educational environment like this class.\n\n\n3 Most in demand skills ranked\n\n\nCode\n# Skill Key Words\nimport pandas as pd\nimport re\nfrom collections import Counter\n\n# 1️⃣ Define the skills you want to look for\nskills_keywords = [\n    \"python\", \"sql\", \"machine learning\", \"cloud\", \"aws\", \"azure\",\n    \"docker\", \"java\", \"excel\", \"r\", \"linux\", \"tableau\", \"power bi\",\n    \"spark\", \"hadoop\", \"sas\", \"javascript\", \"c++\", \"pandas\", \"numpy\"\n]\n\n# 2️⃣ Compile regex patterns for faster matching\npattern_dict = {skill: re.compile(rf\"\\b{re.escape(skill)}\\b\") for skill in skills_keywords}\n\n# 3️⃣ Initialize a counter to store matches\nskill_counts = Counter()\n\n# 4️⃣ Read the CSV in chunks to avoid memory overload\nchunk_size = 10000  # You can adjust this if your EC2 has more memory\nfor chunk in pd.read_csv(\n    \"data/lightcast_cleaned.csv\",\n    usecols=[\"BODY\"],\n    chunksize=chunk_size,\n    on_bad_lines=\"skip\",         # &lt;-- skip broken lines instead of crashing\n    engine=\"python\",             # &lt;-- slower but handles messy text safely\n    encoding=\"utf-8\",            # &lt;-- explicitly set encoding\n    sep=\",\",                     # &lt;-- enforce comma delimiter\n    quoting=3                    # &lt;-- ignore quote mismatches\n):\n    # Drop missing text and convert to lowercase\n    chunk = chunk[\"BODY\"].dropna().str.lower()\n    \n    # For each job description, count skill occurrences\n    for text in chunk:\n        for skill, pattern in pattern_dict.items():\n            skill_counts[skill] += len(pattern.findall(text))\n\n# 5️⃣ Get the top 5 most common skills\ntop_skills = skill_counts.most_common(5)\n\n# 6️⃣ Print results neatly\nimport pandas as pd\n\n# Convert Counter to DataFrame\nskills_df = pd.DataFrame(skill_counts.items(), columns=[\"Skill\", \"Count\"])\n\n# Optional: sort by count descending\nskills_df = skills_df.sort_values(by=\"Count\", ascending=False).reset_index(drop=True)\nskills_df.head(10)\n\n\n\n\n\n\n\n\n\n\nSkill\nCount\n\n\n\n\n0\nsql\n4530\n\n\n1\ncloud\n2638\n\n\n2\npython\n1711\n\n\n3\nexcel\n1494\n\n\n4\ntableau\n1222\n\n\n5\nr\n743\n\n\n6\nazure\n726\n\n\n7\nmachine learning\n672\n\n\n8\nsas\n566\n\n\n9\npower bi\n494\n\n\n\n\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nskills, counts = zip(*skill_counts.most_common(10))\n\n# Make figure taller for vertical bars, and increase quality of graphic\nplt.figure(figsize=(10, 6), dpi=300)\n\n# Unique color per bar\ncolors = plt.cm.tab10(np.linspace(0, 1, len(skills)))\n\n# Plot vertical bars\nbars = plt.bar(skills, counts, color=colors, edgecolor=\"black\")\n\n# Titles and labels\nplt.title(\"Top 10 Most Mentioned Skills in Job Descriptions\", fontsize=14, fontweight=\"bold\", pad=15)\nplt.xlabel(\"Skill\", fontsize=12)\nplt.ylabel(\"Frequency (Number of Mentions)\", fontsize=12)\n\n# Rotate x labels for readability\nplt.xticks(rotation=45, ha=\"right\", fontsize=10)\nplt.yticks(fontsize=10)\n\n# Add values on top of bars\nfor bar in bars:\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(counts)*0.01,\n             f\"{bar.get_height():,}\", ha=\"center\", va=\"bottom\", fontsize=9, color=\"black\")\n\n# Gridlines and layout\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n4 Addressing Skill Gaps\nWhen looking at our groups skill gaps, the group should focus on our weakest points which are Cloud Computing and Machine Learning.\nFor Cloud Computing there are many programs that offer the ability to gain experience and knowledge on platform specific programs such as AWS, Azure, or Google Cloud Platform (GCP). These are just a few examples of ways that our group could continue to expand on their knowledge and grow their analytical toolkit to expand for the future and meet the ever expanding requirements out in the job market. Cloud Computing is a great way to do it, as cloud was the highest noted keyword, indicating that there is increased work going on in the cloud. These programs can also include access to free tiers such as AWS, Azure or GCP to experiment, just like the EC2 we are using for this class currently!\nFor Machine learning our group is below average in technical skill level, with an average skill level of 2 out of 5. While this is mostly due to a lack of exposure, this can be overcome with practical exercises such as Datacamp, which states “Machine learning courses cover algorithms and concepts for enabling computers to learn from data and make decisions without explicit programming. Build your skills in NLP, deep learning, MLOps and more.” (DataCamp (2025)). This is a way to gain practical skills that can be translated into real world careers, but similar to how PowerBI was mentioned above, Machine Learning is ranked on the lower end of the count when looking at Job Descriptions, which may just mean that these are less in demand jobs but not necessarily that they are lower paying jobs. This may not be as valuable a skill to learn as continuing to increase skills such as Python,Cloud, or SQL.\n\n\n5 Comparing Top Job Market Skills\nWhen looking at the most important skill gaps to consider in terms of the job market, we compared our average skills (last column of the heatmap) with the most in demand across all job descriptions within the dataset. This revealed that we should focus on improving our abilities in data analysis, along with python. While our group has had some introductory knowledge into Python in this class there are still advances that could be made in order to continue to grow and become proficient and employable at a high level. Below are two charts that break down the teams skills average skills vs the same skills that are in demand within the data set provided for this assignment.\n(Results of the chart)\nData Analysis…\nPython…\nAssessment …\n\n\n6 Summary\n\n\n\n\n\n\n\n\n7 References\n\nDataCamp. (2025): “Machine Learning Courses | DataCamp,”"
  },
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "1 Looks for the duration of job postings\n\n\nCode\nfrom pyspark.sql.functions import col, to_date, datediff, when\n#| label: duration\n#| include: true\n\n# --- Step 1: Use a flexible date format ---\ndate_cols = [\"POSTED\", \"EXPIRED\", \"MODELED_EXPIRED\"]\nfor colname in date_cols:\n    # Cast to string first in case it's not string\n    df = df.withColumn(colname, col(colname).cast(\"string\"))\n    # Use M/d/yyyy for single-digit month/day\n    df = df.withColumn(colname, to_date(col(colname), \"M/d/yyyy\"))\n\n# --- Step 2: Compute durations safely ---\ndf = df.withColumn(\n    \"DURATION\",\n    when(col(\"DURATION\").isNull(), datediff(\"EXPIRED\", \"POSTED\")).otherwise(col(\"DURATION\"))\n).withColumn(\n    \"MODELED_DURATION\",\n    when(col(\"MODELED_DURATION\").isNull(), datediff(\"MODELED_EXPIRED\", \"POSTED\")).otherwise(col(\"MODELED_DURATION\"))\n)\n\n\n\n\n2 Column Cleaning\n\n\nCode\n# ===============================\n# Full Text Column Cleaning\n# ===============================\n\n# --- Imports ---\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, regexp_replace\n\n# --- Columns to clean ---\ntext_columns = [\n    \"TITLE_RAW\", \"BODY\", \"SKILLS_NAME\",\n    \"SPECIALIZED_SKILLS_NAME\",\n    \"CERTIFICATIONS_NAME\",\n    \"COMMON_SKILLS_NAME\",\n    \"SOFTWARE_SKILLS_NAME\", \"URL\",\n    \"EDUCATION_LEVELS_NAME\", \"LIGHTCAST_SECTORS_NAME\",\n    \"CIP6_NAME\", \"CIP4_NAME\", \"CIP2_NAME\",\n    \"TITLE_NAME\", \"TITLE_CLEAN\",\n    \"COMPANY_NAME\", \"COMPANY_RAW\",\n    \"ONET\", \"ONET_NAME\", \"ONET_2019\", \"ONET_2019_NAME\",\n    \"SOC_2021_2_NAME\", \"SOC_2021_3_NAME\", \"SOC_2021_4_NAME\", \"SOC_2021_5_NAME\",\n    \"LOT_CAREER_AREA_NAME\", \"LOT_OCCUPATION_NAME\", \"LOT_SPECIALIZED_OCCUPATION_NAME\",\n    \"LOT_OCCUPATION_GROUP_NAME\", \"LOT_V6_SPECIALIZED_OCCUPATION_NAME\",\n    \"LOT_V6_OCCUPATION_NAME\", \"LOT_V6_OCCUPATION_GROUP_NAME\", \"LOT_V6_CAREER_AREA_NAME\",\n    \"NAICS_2022_6_NAME\"\n]\n\n# --- Function to clean text columns ---\ndef clean_text_columns(df, columns):\n    \"\"\"\n    Cleans text-heavy columns by:\n    - Removing brackets, quotes, and newlines\n    - Replacing multiple spaces with single space\n    - Standardizing commas with proper spacing\n    \"\"\"\n    pattern_cleanup = r'[\\[\\]\\n{}\"]'  # remove brackets, newlines, braces, quotes\n    for c in columns:\n        df = df.withColumn(c, regexp_replace(col(c), pattern_cleanup, \"\"))\n        df = df.withColumn(c, regexp_replace(col(c), r'\\s{2,}', ' '))  # multiple spaces → 1 space\n        df = df.withColumn(c, regexp_replace(col(c), r'\\s*,\\s*', ', '))  # standardize commas\n    return df\n\n# --- Apply cleaning ---\ndf = clean_text_columns(df, text_columns)\n\n# --- Preview cleaned text ---\n# df.select(text_columns).show(5, truncate=100)\n\n\n\n\n3 Targeted column-specific cleaning\n\n\nCode\nfrom pyspark.sql.functions import col, regexp_extract, when, regexp_replace\n#| label: targeted_cleaning\n#| include: true\n\n# Education levels → keep digits\ndf = df.withColumn(\"EDUCATION_LEVELS\", regexp_extract(\"EDUCATION_LEVELS\", r'(\\d+)', 1))\n\n# Location cleanup\ndf = df.withColumn(\"LOCATION\", regexp_replace(col(\"LOCATION\"), r\"\\s*\\n\\s*\", \" \"))\ndf = df.withColumn(\"LOCATION\", regexp_replace(col(\"LOCATION\"), r\"[{}]\", \"\"))\n\n# Standardize remote work labels\ndf = df.withColumn(\n    \"REMOTE_TYPE_NAME\",\n    when(col(\"REMOTE_TYPE_NAME\").isin(\"[None]\", \"Not Remote\") | col(\"REMOTE_TYPE_NAME\").isNull(), \"On-Site\")\n    .when(col(\"REMOTE_TYPE_NAME\") == \"Hybrid Remote\", \"Hybrid\")\n    .when(col(\"REMOTE_TYPE_NAME\") == \"Remote\", \"Remote\")\n    .otherwise(col(\"REMOTE_TYPE_NAME\"))\n)\n\n\n\n\n4 Save the cleaned dataset"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 About This Project\n\n1.0.1 Exploring the future of work through data, collaboration, and curiosity.\nThe purpose of this project is to explore how job seekers can strategically position themselves in the evolving labor market of 2025. Rather than analyzing trends from a recruiter’s perspective, our team approached this challenge as future professionals navigating a landscape shaped by AI adoption, remote work, shifting salary norms, and gender-based employment patterns.Using real-world data from Lightcast and other labor market sources, we applied data science, analytics, and visualization techniques to investigate key workforce themes. Our goal was to uncover actionable insights that inform personal career planning and highlight the skills most in demand across industries. This website presents our findings, reflections, and recommendations—designed not only to fulfill academic objectives, but to serve as a practical roadmap for anyone preparing to enter or pivot within today’s dynamic job market."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Business Analytics, Data Science and Machine Learning Trends",
    "section": "",
    "text": "The field of data science, business analytics and machine learning has been growing very rapidly in the past few years due to increase in use of artificial intelligence and automation across sectors. With this emergence of new tools and platforms, companies are constantly updating their hiring criteria. Now more than before, job roles are asking for specific skillsets including both technical and analytical abilities. Therefore, it is important to understand what skills are most in-demand at present, how job descriptions have changed in recent times and which job industries are offering the most opportunities in these domain.\n\n\n\nRecent studies are showing that employers are now giving more importance to skill-based hiring especially in data-related roles. (Bone, Ehlinger, and Stephany (2025)) found that in AI-related job postings, the need for formal degrees is slowly reducing while practical skills like machine learning and NLP are becoming more valued. (Mäkelä and Stephany (2024)) also pointed out that roles demanding AI expertise now also look for soft skills like adaptability and teamwork showing that a balance of technical and human skills is becoming necessary. These findings suggest that job descriptions are evolving to reflect this shift in priorities.\nFurther, our study reveals that hiring is strong in technology, healthcare and financial sectors where the use of data is becoming highly critical. These trends clearly indicates that both education and job markets must keep evolving to stay aligned with real-world demands."
  },
  {
    "objectID": "index.html#rationale",
    "href": "index.html#rationale",
    "title": "Business Analytics, Data Science and Machine Learning Trends",
    "section": "",
    "text": "The field of data science, business analytics and machine learning has been growing very rapidly in the past few years due to increase in use of artificial intelligence and automation across sectors. With this emergence of new tools and platforms, companies are constantly updating their hiring criteria. Now more than before, job roles are asking for specific skillsets including both technical and analytical abilities. Therefore, it is important to understand what skills are most in-demand at present, how job descriptions have changed in recent times and which job industries are offering the most opportunities in these domain."
  },
  {
    "objectID": "index.html#literature-review",
    "href": "index.html#literature-review",
    "title": "Business Analytics, Data Science and Machine Learning Trends",
    "section": "",
    "text": "Recent studies are showing that employers are now giving more importance to skill-based hiring especially in data-related roles. (Bone, Ehlinger, and Stephany (2025)) found that in AI-related job postings, the need for formal degrees is slowly reducing while practical skills like machine learning and NLP are becoming more valued. (Mäkelä and Stephany (2024)) also pointed out that roles demanding AI expertise now also look for soft skills like adaptability and teamwork showing that a balance of technical and human skills is becoming necessary. These findings suggest that job descriptions are evolving to reflect this shift in priorities.\nFurther, our study reveals that hiring is strong in technology, healthcare and financial sectors where the use of data is becoming highly critical. These trends clearly indicates that both education and job markets must keep evolving to stay aligned with real-world demands."
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "1 Overview\nIn this section, we examine job postings across Business Analytics, Data Science, and Machine Learning roles to identify key patterns before modeling. We explore salary, experience, remote work trends, and skill demand to guide the regression and classification stages.\nWe focused on: - Job distribution across industries and roles\n- Salary variation by experience and remote type\n- Skill demand frequency and specialization\n- Emerging data science and analytics trends\n\n\n2 Load and Prepare Data\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nimport plotly.io as pio\nimport json\nimport re\nfrom collections import Counter\n\npio.templates.default = \"plotly_white\"\npio.renderers.default = \"iframe_connected\"\n\n# === Load data from CSV ===\ndf = pd.read_csv(\"data/lightcast_job_postings.csv\", low_memory=False)\nprint(f\"Dataset loaded: {len(df):,} rows, {len(df.columns)} columns\")\n\n# --- Detect & drop duplicate columns ---\n# --- Detect & fully clean duplicate-like columns ---\n# Normalize column names: strip whitespace and hidden characters\ndf.columns = df.columns.str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n\n# Collapse exact duplicates after cleanup\nbefore_cols = len(df.columns)\ndf = df.loc[:, ~df.columns.duplicated()]\nafter_cols = len(df.columns)\n\nprint(f\"🧹 Cleaned column names: removed {before_cols - after_cols} duplicate(s).\")\nprint(\"Unique columns now:\", len(df.columns))\n\n# --- Convert numeric columns safely ---\nfor col in [\"SALARY_FROM\", \"SALARY_TO\", \"MIN_YEARS_EXPERIENCE\", \"MAX_YEARS_EXPERIENCE\"]:\n    if col in df.columns:\n        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n\n# --- Compute average salary (avoid string concat) ---\nif {\"SALARY_FROM\", \"SALARY_TO\"}.issubset(df.columns):\n    df[\"Average_Salary\"] = df[[\"SALARY_FROM\", \"SALARY_TO\"]].mean(axis=1, skipna=True)\n\n# --- Clean and rename (safely) ---\nrename_map = {\n    \"REMOTE_TYPE_NAME\": \"REMOTE_GROUP\",\n    \"STATE_NAME\": \"STATE\",\n    \"LOT_V6_OCCUPATION_GROUP_NAME\": \"ROLE_GROUP\"\n}\n\n# Only rename columns that won't create duplicates\nfor old, new in rename_map.items():\n    if old in df.columns and new not in df.columns:\n        df.rename(columns={old: new}, inplace=True)\n    elif old in df.columns and new in df.columns:\n        print(f\"⚠️ Skipping rename '{old}' → '{new}' to avoid duplicate column name.\")\n\n\n# --- Drop invalid rows early ---\nif \"Average_Salary\" in df.columns:\n    df = df[df[\"Average_Salary\"].notna() & (df[\"Average_Salary\"] &gt; 0)]\n\n# --- Downsample if dataset is large ---\nif len(df) &gt; 5000:\n    df = df.sample(5000, random_state=42)\n\nprint(f\"✅ Loaded {len(df)} rows safely with {len(df.columns)} unique columns.\")\n\n# --- Lightweight histogram check ---\nfig = px.histogram(\n    df,\n    x=\"Average_Salary\",\n    nbins=25,\n    color_discrete_sequence=[\"#187145\"],\n    title=\"Average Salary Distribution (sample)\"\n)\nfig.update_layout(template=\"plotly_white\")\nfig\n\n\nDataset loaded: 72,498 rows, 131 columns\n🧹 Cleaned column names: removed 0 duplicate(s).\nUnique columns now: 131\n⚠️ Skipping rename 'STATE_NAME' → 'STATE' to avoid duplicate column name.\n✅ Loaded 5000 rows safely with 132 unique columns.\n\n\n\n\n\n\n\nCode\nprint(sorted(df.columns))\n\n\n['ACTIVE_SOURCES_INFO', 'ACTIVE_URLS', 'Average_Salary', 'BODY', 'CERTIFICATIONS', 'CERTIFICATIONS_NAME', 'CIP2', 'CIP2_NAME', 'CIP4', 'CIP4_NAME', 'CIP6', 'CIP6_NAME', 'CITY', 'CITY_NAME', 'COMMON_SKILLS', 'COMMON_SKILLS_NAME', 'COMPANY', 'COMPANY_IS_STAFFING', 'COMPANY_NAME', 'COMPANY_RAW', 'COUNTY', 'COUNTY_INCOMING', 'COUNTY_NAME', 'COUNTY_NAME_INCOMING', 'COUNTY_NAME_OUTGOING', 'COUNTY_OUTGOING', 'DUPLICATES', 'DURATION', 'EDUCATION_LEVELS', 'EDUCATION_LEVELS_NAME', 'EMPLOYMENT_TYPE', 'EMPLOYMENT_TYPE_NAME', 'EXPIRED', 'ID', 'IS_INTERNSHIP', 'LAST_UPDATED_DATE', 'LAST_UPDATED_TIMESTAMP', 'LIGHTCAST_SECTORS', 'LIGHTCAST_SECTORS_NAME', 'LOCATION', 'LOT_CAREER_AREA', 'LOT_CAREER_AREA_NAME', 'LOT_OCCUPATION', 'LOT_OCCUPATION_GROUP', 'LOT_OCCUPATION_GROUP_NAME', 'LOT_OCCUPATION_NAME', 'LOT_SPECIALIZED_OCCUPATION', 'LOT_SPECIALIZED_OCCUPATION_NAME', 'LOT_V6_CAREER_AREA', 'LOT_V6_CAREER_AREA_NAME', 'LOT_V6_OCCUPATION', 'LOT_V6_OCCUPATION_GROUP', 'LOT_V6_OCCUPATION_NAME', 'LOT_V6_SPECIALIZED_OCCUPATION', 'LOT_V6_SPECIALIZED_OCCUPATION_NAME', 'MAX_EDULEVELS', 'MAX_EDULEVELS_NAME', 'MAX_YEARS_EXPERIENCE', 'MIN_EDULEVELS', 'MIN_EDULEVELS_NAME', 'MIN_YEARS_EXPERIENCE', 'MODELED_DURATION', 'MODELED_EXPIRED', 'MSA', 'MSA_INCOMING', 'MSA_NAME', 'MSA_NAME_INCOMING', 'MSA_NAME_OUTGOING', 'MSA_OUTGOING', 'NAICS2', 'NAICS2_NAME', 'NAICS3', 'NAICS3_NAME', 'NAICS4', 'NAICS4_NAME', 'NAICS5', 'NAICS5_NAME', 'NAICS6', 'NAICS6_NAME', 'NAICS_2022_2', 'NAICS_2022_2_NAME', 'NAICS_2022_3', 'NAICS_2022_3_NAME', 'NAICS_2022_4', 'NAICS_2022_4_NAME', 'NAICS_2022_5', 'NAICS_2022_5_NAME', 'NAICS_2022_6', 'NAICS_2022_6_NAME', 'ONET', 'ONET_2019', 'ONET_2019_NAME', 'ONET_NAME', 'ORIGINAL_PAY_PERIOD', 'POSTED', 'REMOTE_GROUP', 'REMOTE_TYPE', 'ROLE_GROUP', 'SALARY', 'SALARY_FROM', 'SALARY_TO', 'SKILLS', 'SKILLS_NAME', 'SOC_2', 'SOC_2021_2', 'SOC_2021_2_NAME', 'SOC_2021_3', 'SOC_2021_3_NAME', 'SOC_2021_4', 'SOC_2021_4_NAME', 'SOC_2021_5', 'SOC_2021_5_NAME', 'SOC_2_NAME', 'SOC_3', 'SOC_3_NAME', 'SOC_4', 'SOC_4_NAME', 'SOC_5', 'SOC_5_NAME', 'SOFTWARE_SKILLS', 'SOFTWARE_SKILLS_NAME', 'SOURCES', 'SOURCE_TYPES', 'SPECIALIZED_SKILLS', 'SPECIALIZED_SKILLS_NAME', 'STATE', 'STATE_NAME', 'TITLE', 'TITLE_CLEAN', 'TITLE_NAME', 'TITLE_RAW', 'URL']\n\n\n\n\nCode\nfig = px.histogram(\n    df, x=\"Average_Salary\", nbins=40,\n    color_discrete_sequence=[\"#187145\"],\n    title=\"Distribution of Average Salaries\"\n)\nfig.update_layout(\n    xaxis_title=\"Average Salary ($)\",\n    yaxis_title=\"Number of Job Postings\",\n    template=\"plotly_white\"\n)\nfig\n\n\n\n\n\n\n2.0.1 Salary Distribution and Outliers\nSalaries are right-skewed — most roles pay under $150K, but select senior and ML-focused positions exceed $200K, highlighting outlier opportunities for experienced professionals.\n\n\nCode\nfig = px.scatter(\n    df,\n    x=\"MIN_YEARS_EXPERIENCE\",\n    y=\"Average_Salary\",\n    color=\"REMOTE_GROUP\",\n    trendline=\"ols\",\n    title=\"Salary vs. Minimum Experience by Remote Type\",\n    height=500\n)\nfig.update_layout(template=\"plotly_white\")\nfig\n\n\n\n\n\n\n\n2.0.2 Salary vs Experience\nSalary shows a positive correlation with experience, particularly for hybrid and remote roles — suggesting advanced or flexible positions are often compensated higher.\n\n\nCode\nfig = px.box(\n    df,\n    x=\"ROLE_GROUP\",\n    y=\"Average_Salary\",\n    color=\"ROLE_GROUP\",\n    color_discrete_sequence=[\"#187145\", \"#45A274\", \"#79C99E\"],\n    title=\"Salary Comparison Across Role Categories\"\n)\nfig.update_layout(template=\"plotly_white\")\nfig\n\n\n\n\n\n\n\n2.0.3 Salary by Role Category (BA / DS / ML)\nMachine Learning roles exhibit the highest median salaries, followed by Data Science and Business Analytics. This validates using ROLE_GROUP as a key feature in regression modeling.\n\n\nCode\nfig = px.box(\n    df,\n    x=\"REMOTE_GROUP\",\n    y=\"Average_Salary\",\n    color=\"REMOTE_GROUP\",\n    color_discrete_sequence=[\"#45A274\", \"#79C99E\", \"#A7D9C9\"],\n    title=\"Salary Distribution by Remote Work Type\"\n)\nfig.update_layout(template=\"plotly_white\")\nfig\n\n\n\n\n\n\n\n2.0.4 Remote Work vs Salary Trends\nRemote roles tend to offer higher median salaries compared to onsite jobs, supporting modern hybrid compensation trends in data-driven fields.\n\n\nCode\n# --- Top Skills Visualization (Safe Version) ---\n\nskills_column = None\nfor col in [\"SKILLS_NAME\", \"COMMON_SKILLS_NAME\", \"SPECIALIZED_SKILLS_NAME\", \"SOFTWARE_SKILLS_NAME\"]:\n    if col in df.columns:\n        skills_column = col\n        break\n\nif skills_column:\n    print(f\"✅ Using '{skills_column}' for skill frequency analysis.\")\n    \n    skills_flat = [\n        s.strip()\n        for sublist in df[skills_column].dropna().astype(str).str.split(',')\n        for s in sublist if s.strip()\n    ]\n\n    skill_counts = pd.DataFrame(\n        Counter(skills_flat).most_common(15),\n        columns=[\"Skill\", \"Count\"]\n    )\n\n    fig = px.bar(\n        skill_counts,\n        x=\"Skill\",\n        y=\"Count\",\n        title=f\"Top 15 Most Frequent Skills ({skills_column})\",\n        color_discrete_sequence=[\"#187145\"]\n    )\n    fig.update_layout(template=\"plotly_white\")\n    fig\nelse:\n    print(\"⚠️ No skill-related column found — skipping skills frequency plot.\")\n\n\n✅ Using 'SKILLS_NAME' for skill frequency analysis.\n\n\n\n\n2.0.5 Top Skills Frequency\nPython, SQL, and Machine Learning dominate job descriptions, indicating their critical role in employability and forming the foundation for the modeling phase.\n\n\nCode\ncorr_cols = [\"Average_Salary\", \"MIN_YEARS_EXPERIENCE\", \"MAX_YEARS_EXPERIENCE\"]\ncorr = df[corr_cols].corr().round(2)\n\nfig = ff.create_annotated_heatmap(\n    z=corr.values,\n    x=corr.columns.tolist(),\n    y=corr.columns.tolist(),\n    colorscale='greens',\n    showscale=True\n)\nfig.update_layout(title=\"Feature Correlation Matrix\", template=\"plotly_white\", height=450)\nfig\n\n\n\n\n\n\n\n2.0.6 Correlation Heatmap\nSalary correlates moderately with both minimum and maximum experience, confirming their inclusion in regression modeling. We’ll rely on additional categorical encodings (e.g., REMOTE_GROUP, ROLE_GROUP) for predictive depth."
  }
]
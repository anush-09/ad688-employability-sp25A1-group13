---
title: "Business Analytics, Data Science and Machine Learning Trends"
author: "Anu Sharma, Cindy Guzman, Gavin Boss"
date: "2025-09-10"
format:
  html:
    bibliography: references.bib
    csl: csl/econometrica.csl
    toc: true
execute:
  echo: false
  warning: false
freeze: auto
---

# Overview

This analysis explores trends in Business Analytics, Data Science, and Machine Learning job postings by focusing on the **skills required** for these roles. We analyze how different skill combinations impact salary, remote work opportunities, and career paths.

# Data Loading and Setup

```{python}
# | echo: true
# | eval: true

import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.io as pio
import json
import re
from collections import Counter

pio.templates.default = "plotly_white"
pio.renderers.default = "notebook"

# Load data from csv
df = pd.read_csv("../assignment-04-anush-09/data/lightcast_job_postings.csv", low_memory=False)
print(f"Dataset loaded: {len(df):,} rows, {len(df.columns)} columns")

df.info()

# Skills-related columns identified through manual inspection of schema
skills_columns = ['SKILLS', 'SKILLS_NAME', 'SPECIALIZED_SKILLS', 'SPECIALIZED_SKILLS_NAME',
                  'SOFTWARE_SKILLS', 'SOFTWARE_SKILLS_NAME', 'CERTIFICATIONS', 'CERTIFICATIONS_NAME']
print("\nSkills-related columns identified through schema:")
for col in skills_columns:
    if col in df.columns:
        print(f"- {col}")
```

# Skills Data Preprocessing

```{python}
#| echo: true
#| eval: true

# Apply filters
df_filtered = df.dropna(subset=['SALARY', 'TITLE'])

# Convert salary to numeric and filter
df_filtered['SALARY'] = pd.to_numeric(df_filtered['SALARY'], errors='coerce')
df_filtered = df_filtered[df_filtered['SALARY'] > 0]

print(f"Records after filtering: {len(df_filtered):,}")

df_skills = df_filtered.copy()

# Focus on key ML/Data Science skills. We identifief some key skills for
# ML/DS roles manually.
key_skills =  [
        'Python (Programming Language)',
        'R (Programming Language)',
        'SQL (Programming Language)',
        'Machine Learning',
        'Data Science',
        'Data Analysis',
        'Statistics',
        'Artificial Intelligence',
        'TensorFlow',
        'PyTorch (Machine Learning Library)',
        'Pandas (Python Package)',
        'NumPy (Python Package)',
        'Scikit-Learn (Python Package)',
        'Big Data',
        'Apache Spark',
        'Apache Hadoop',
        'Amazon Web Services',
        'Microsoft Azure',
        'Google Cloud Platform (Gcp)',
        'Data Visualization',
        'Tableau (Business Intelligence Software)',
        'Power BI',
        'Natural Language Processing (NLP)',
        'Computer Vision',
        'Deep Learning'
    ]

print(f"Using focused {len(key_skills)} ML/Data Science skills for analysis")

# Create binary features for each key skill.
for skill in key_skills:
    # Clean skill name for column naming
    # Eg: R (Programming Language) --> has_r_programming_language
    skill_col_name = f'has_{skill.lower().replace(" ", "_").replace("-", "_").replace("(", "").replace(")", "")}'


    df_skills[skill_col_name] = (
        df_skills['SKILLS_NAME'].str.contains(skill, case=False, na=False, regex=False) |
        df_skills['SOFTWARE_SKILLS_NAME'].str.contains(skill, case=False, na=False, regex=False) |
        df_skills['SPECIALIZED_SKILLS_NAME'].str.contains(skill, case=False, na=False, regex=False)
    ).astype(int)

print("Binary skill features created")

# Create ML/DS role indicator using focused skills
core_ml_skills = [
    'has_machine_learning', 'has_artificial_intelligence', 'has_tensorflow', 'has_pytorch_machine_learning_library',
    'has_deep_learning', 'has_natural_language_processing_nlp', 'has_computer_vision'
]

core_ds_skills = [
    'has_python_programming_language', 'has_r_programming_language', 'has_statistics', 'has_data_analysis',
    'has_data_science', 'has_pandas_python_package', 'has_numpy_python_package',
    'has_scikit_learn_python_package'
]

# Role indicators
df_skills['is_ml_role'] = (
    (df_skills[core_ml_skills].sum(axis=1) > 0)
).astype(int)
# Assuming that if there are least 2 DS-related skills, then it is a
# DS role.
df_skills['is_ds_role'] = (
    (df_skills[core_ds_skills].sum(axis=1) > 1)
).astype(int)

df_skills['is_ml_ds_role'] = ((df_skills['is_ml_role'] == 1) | (df_skills['is_ds_role'] == 1)).astype(int)


# Remote work indicator
df_skills['is_remote'] = df_skills['REMOTE_TYPE'].fillna(0).astype(int)
df_skills['experience_years'] = df_skills['MIN_YEARS_EXPERIENCE'].fillna(0)

df_final = df_skills

# # Check which skills actually exist in the dataframe
# available_core_ml = [skill for skill in core_ml_skills if skill in df_skills.columns]
# available_ds_skills = [skill for skill in data_science_skills if skill in df_skills.columns]

# print(f"Available core ML skills ({len(available_core_ml)}): {available_core_ml}")
# print(f"Available DS skills ({len(available_ds_skills)}): {available_ds_skills}")

# # ML role if has core ML skills OR (Python + Statistics/Data Analysis)
# if available_core_ml:
#     df_skills['has_core_ml'] = df_skills[available_core_ml].sum(axis=1) > 0
# else:
#     df_skills['has_core_ml'] = False

# if available_ds_skills and 'has_python_programming_language' in df_skills.columns:
#     # Check for Python + (Statistics OR Data Analysis OR Data Science)
#     stats_cols = [col for col in ['has_statistics', 'has_data_analysis', 'has_data_science'] if col in df_skills.columns]
#     if stats_cols:
#         df_skills['has_ds_combo'] = (df_skills['has_python_programming_language'] == 1) & (df_skills[stats_cols].sum(axis=1) > 0)
#     else:
#         df_skills['has_ds_combo'] = False
# else:
#     df_skills['has_ds_combo'] = False

# df_skills['is_ml_role'] = (df_skills['has_core_ml'] | df_skills['has_ds_combo']).astype(int)

# # Create remote work indicator
# df_skills['is_remote'] = df_skills['REMOTE_TYPE'].fillna(0).astype(int)
# df_skills['experience_years'] = df_skills['MIN_YEARS_EXPERIENCE'].fillna(0)

# df_final = df_skills

print(f"Final dataset size: {len(df_final):,}")
print(f"ML/Data Science roles identified: {df_final['is_ml_ds_role'].sum():,}")
```

```{python}
#| echo: true
#| eval: false

# Filter title and salary.
df_filtered = df.dropna(subset=['SALARY', 'TITLE'])

# Convert salary to numeric and filter null
df_filtered['SALARY'] = pd.to_numeric(df_filtered['SALARY'], errors='coerce')
df_filtered = df_filtered[df_filtered['SALARY'] > 0]

print(f"Records after filtering: {len(df_filtered):,}")
df_skills = df_filtered.copy()

# Creating pre-defined key skills.
# Extracting them by going through dataset results in long execution
# time, so we defined them manually for successive runs.
key_skills = [
    'Python', 'SQL', 'Machine Learning', 'R', 'Java', 'JavaScript', 'Tableau', 'Power BI',
    'AWS', 'Azure', 'Google Cloud', 'Docker', 'Kubernetes', 'Spark', 'Hadoop',
    'TensorFlow', 'PyTorch', 'Scikit-learn', 'Pandas', 'NumPy', 'Matplotlib',
    'Excel', 'Statistics', 'Data Analysis', 'Deep Learning', 'NLP', 'Computer Vision'
]

print(f"Using predefined skill list with {len(key_skills)} key skills for "
      "efficient processing...")

# Creating binary features for each key skill
for skill in key_skills:
    # Checking if skill appears in any of the skill columns
    skill_col_name = f'has_{skill.lower().replace(" ", "_").replace("-", "_")}'
    df_skills[skill_col_name] = (
        df_skills['SKILLS_NAME'].str.contains(skill, case=False, na=False) |
        df_skills['SOFTWARE_SKILLS_NAME'].str.contains(skill, case=False, na=False) |
        df_skills['SPECIALIZED_SKILLS_NAME'].str.contains(skill, case=False, na=False)
    ).astype(int)

# # Creating ML/Data Science role indicator using binary features.
# core_ml_skills = ['has_machine_learning', 'has_tensorflow', 'has_pytorch',
#                   'has_deep_learning', 'has_nlp', 'has_computer_vision']
# data_science_skills = ['has_python', 'has_r', 'has_statistics',
#                        'has_data_analysis']

# # ML role if has core ML skills OR (Python + Statistics/Data Analysis)
# df_skills['has_core_ml'] = df_skills[core_ml_skills].sum(axis=1) > 0
# df_skills['has_ds_combo'] = (df_skills['has_python'] == 1) & (df_skills[['has_statistics', 'has_data_analysis']].sum(axis=1) > 0)
# df_skills['is_ml_role'] = (df_skills['has_core_ml'] | df_skills['has_ds_combo']).astype(int)

# ML roles should typically have these skills.
core_ml_skills = [
    'has_machine_learning', 'has_tensorflow', 'has_pytorch',
    'has_deep_learning', 'has_nlp', 'has_computer_vision',
    'has_scikit_learn', 'has_spark'
]

# Data science roles typically require this.
core_ds_skills = [
    'has_python', 'has_r', 'has_statistics', 'has_data_analysis',
    'has_pandas', 'has_numpy', 'has_matplotlib', 'has_excel'
]

# Role indicators
df_skills['is_ml_role'] = (
    (df_skills[core_ml_skills].sum(axis=1) > 0)
).astype(int)
# Assuming that if there are least 2 DS-related skills, then it is a
# DS role.
df_skills['is_ds_role'] = (
    (df_skills[core_ds_skills].sum(axis=1) > 1)
).astype(int)

df_skills['is_ml_ds_role'] = ((df_skills['is_ml_role'] == 1) | (df_skills['is_ds_role'] == 1)).astype(int)


# Remote work indicator
df_skills['is_remote'] = df_skills['REMOTE_TYPE'].fillna(0).astype(int)
df_skills['experience_years'] = df_skills['MIN_YEARS_EXPERIENCE'].fillna(0)

df_final = df_skills

print(f"Final dataset size: {len(df_final):,}")
print(f"ML/Data Science roles identified: {df_final['is_ml_ds_role'].sum():,}")
```

# Feature Engineering for ML
```{python}
#| echo: true
#| eval: true

# Just prepare the modeling dataset
modeling_cols = ['SALARY', 'is_ml_ds_role', 'is_remote', 'experience_years'] + \
               [col for col in df_final.columns if col.startswith('has_')]

df_modeling = df_final[modeling_cols].copy()

print("Features for modeling:")
print(f"Dataset shape: {df_modeling.shape}")
print(f"Columns: {list(df_modeling.columns)}")
print(f"Missing values: {df_modeling.isnull().sum().sum()}")
```

# Unsupervised Learning: KMeans Clustering Based on Skills

```{python}
#| echo: true
#| eval: true

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, f1_score, confusion_matrix, classification_report

# Prepare features for clustering using skills and other features
skill_feature_cols = [col for col in df_modeling.columns if col.startswith('has_')]
print(f"Available skill features: {len(skill_feature_cols)}")

# Base clustering features
clustering_features = skill_feature_cols + ['experience_years', 'is_remote']

# Encode ONET and NAICS6.
le_onet = LabelEncoder()
df_modeling['onet_encoded'] = le_onet.fit_transform(df_final['ONET'].fillna('Unknown'))
clustering_features.append('onet_encoded')

le_naics = LabelEncoder()
df_modeling['naics_encoded'] = le_naics.fit_transform(df_final['NAICS6'].fillna('Unknown'))
clustering_features.append('naics_encoded')

# Prepare clustering data
X_cluster = df_modeling[clustering_features].fillna(0)

# Scale features
scaler_cluster = StandardScaler()
X_cluster_scaled = scaler_cluster.fit_transform(X_cluster)

# KMeans clustering
kmeans = KMeans(n_clusters=6, random_state=42, n_init=10)
clusters = kmeans.fit_predict(X_cluster_scaled)
df_modeling['cluster'] = clusters

print("Skills based clustering completed")
print("Cluster centers:")
for i, center in enumerate(kmeans.cluster_centers_):
    print(f"Cluster {i}: {center}")
```

```{python}
#| echo: true
#| eval: true

# Analyze clustering.
cluster_summary = df_modeling.groupby('cluster').agg({
    'SALARY': ['count', 'mean'],
    'is_ml_ds_role': 'mean',
    'is_remote': 'mean',
    'experience_years': 'mean'
}).round(2)

cluster_summary.columns = ['count', 'avg_salary', 'ml_role_percentage',
                           'remote_percentage', 'avg_experience']
cluster_summary = cluster_summary.reset_index()

print("Skills based Cluster Summary:")
print(cluster_summary)

# Visualize cluster characteristics.
fig = make_subplots(
    rows=2, cols=3,
    subplot_titles=('Cluster Size', 'Average Salary', 'ML Role %',
                   'Remote Work %', 'Avg Experience', 'Salary Distribution'),
    specs=[[{"type": "bar"}, {"type": "bar"}, {"type": "bar"}],
           [{"type": "bar"}, {"type": "bar"}, {"type": "scatter"}]]
)

fig.add_trace(go.Bar(x=cluster_summary['cluster'], y=cluster_summary['count'], name="Count"), row=1, col=1)
fig.add_trace(go.Bar(x=cluster_summary['cluster'], y=cluster_summary['avg_salary'], name="Avg Salary"), row=1, col=2)
fig.add_trace(go.Bar(x=cluster_summary['cluster'], y=cluster_summary['ml_role_percentage'], name="ML %"), row=1, col=3)
fig.add_trace(go.Bar(x=cluster_summary['cluster'], y=cluster_summary['remote_percentage'], name="Remote %"), row=2, col=1)
fig.add_trace(go.Bar(x=cluster_summary['cluster'], y=cluster_summary['avg_experience'], name="Experience"), row=2, col=2)

# Salary distribution by cluster.
fig.add_trace(
    go.Scatter(
        x=df_modeling['cluster'],
        y=df_modeling['SALARY'],
        mode='markers',
        opacity=0.6,
        name="Jobs"
    ),
    row=2, col=3
)

fig.update_layout(
    height=600,
    showlegend=False,
    template="plotly_white",
    title_text="Skills-Based KMeans Clustering Results"
)
fig.show()
```

# Supervised Learning: Multiple Regression

```{python}
#| echo: true
#| eval: true

# Identify regression features.
regression_features = skill_feature_cols + ['experience_years', 'is_remote', 'is_ml_ds_role']

# Prepare regression data using salary as the target variable
X_reg = df_modeling[regression_features].fillna(0)
y_reg = df_modeling['SALARY']

X_train, X_test, y_train, y_test = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)

print(f"Training set size: {len(X_train):,}")
print(f"Test set size: {len(X_test):,}")

# Scale features
scaler_reg = StandardScaler()
X_train_scaled = scaler_reg.fit_transform(X_train)
X_test_scaled = scaler_reg.transform(X_test)

# Multiple Linear Regression
lr = LinearRegression()
lr.fit(X_train_scaled, y_train)

# Random Forest Regression
rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)
rf_reg.fit(X_train_scaled, y_train)

print("Skills based regression models training completed")
```

```{python}
#| echo: true
#| eval: true

# Evaluate regression models
# Linear Regression predictions
y_pred_lr = lr.predict(X_test_scaled)
rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))
r2_lr = r2_score(y_test, y_pred_lr)

# Random Forest predictions
y_pred_rf = rf_reg.predict(X_test_scaled)
rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))
r2_rf = r2_score(y_test, y_pred_rf)

print("Skills-based Regression Model Performance:")
print(f"Linear Regression - RMSE: ${rmse_lr:,.2f}, R²: {r2_lr:.4f}")
print(f"Random Forest - RMSE: ${rmse_rf:,.2f}, R²: {r2_rf:.4f}")

# Feature importance for Random Forest
# Only use features that actually exist in the model
actual_feature_names = [col for col in regression_features if col in X_train.columns]
importances = rf_reg.feature_importances_

# Visualize feature importance
fig = px.bar(x=actual_feature_names, y=importances,
             title="Skills Impact on Salary (Random Forest Feature Importance)",
             labels={'x': 'Features', 'y': 'Importance'})
fig.update_layout(template="plotly_white", xaxis_tickangle=-45)
fig.show()

# Top skills by salary impact
skill_importance = list(zip(actual_feature_names, importances))
skill_importance.sort(key=lambda x: x[1], reverse=True)
print("\nTop skills by salary impact:")
for skill, importance in skill_importance[:10]:
    print(f"{skill}: {importance:.4f}")
```

# Supervised Learning: Classification Using Skills (Random Forest Only)

```{python}
#| echo: true
#| eval: true

# Prepare features for classification.
classification_features = skill_feature_cols + ['experience_years', 'is_remote']

# Prepare classification data
X_clf = df_modeling[classification_features].fillna(0)
y_clf = df_modeling['is_ml_ds_role']

# Train/test split for classification
X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X_clf, y_clf, test_size=0.2, random_state=42)

# Scale features
scaler_clf = StandardScaler()
X_train_clf_scaled = scaler_clf.fit_transform(X_train_clf)
X_test_clf_scaled = scaler_clf.transform(X_test_clf)

# Random Forest Classification
rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)
rf_clf.fit(X_train_clf_scaled, y_train_clf)

print("Skills-based classification model trained successfully!")
```

```{python}
#| echo: true
#| eval: true
#|
# Random Forest predictions
y_pred_rf_clf = rf_clf.predict(X_test_clf_scaled)
accuracy_rf = accuracy_score(y_test_clf, y_pred_rf_clf)
f1_rf = f1_score(y_test_clf, y_pred_rf_clf)

print("Skills based Classification Model Performance:")
print(f"Random Forest - Accuracy: {accuracy_rf:.4f}, F1 Score: {f1_rf:.4f}")

# Confusion Matrix for Random Forest
cm = confusion_matrix(y_test_clf, y_pred_rf_clf)

# Visualize confusion matrix
fig = px.imshow(cm, text_auto=True, aspect="auto",
                title="Confusion Matrix - ML/DS Role Classification",
                labels=dict(x="Predicted", y="Actual"),
                color_continuous_scale="Blues")
fig.update_layout(template="plotly_white")
fig.show()

print("Classification Report:")
print(classification_report(y_test_clf, y_pred_rf_clf))

# Feature importance for classification
# Only use features that actually exist in the classification model
clf_actual_feature_names = [col for col in classification_features if col in X_train_clf.columns]
clf_importances = rf_clf.feature_importances_

# Visualize classification feature importance
fig = px.bar(x=clf_actual_feature_names, y=clf_importances,
             title="Skills Impact on ML/Data Science Role Classification",
             labels={'x': 'Features', 'y': 'Importance'})
fig.update_layout(template="plotly_white", xaxis_tickangle=-45)
fig.show()
```

